{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5Ejdmu_pknp",
    "outputId": "58a83bad-4ec0-4931-fd80-69f7aa956da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits shape: torch.Size([1, 5, 50257])\n",
      "Keys shape: torch.Size([1, 5, 768])\n",
      "Values shape: torch.Size([1, 5, 768])\n",
      "Memory augmented shape: torch.Size([1, 5, 768])\n",
      "Generated text: tensor([[    1,     2,     3,     4,     5,   155, 27191, 22686, 22686, 22686,\n",
      "         22686, 22686, 14755, 35641, 35641]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "class CachedMemoryBank(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, memory_dim):\n",
    "        super(CachedMemoryBank, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.memory_key = nn.Linear(embedding_dim, memory_dim)\n",
    "        self.memory_value = nn.Linear(embedding_dim, memory_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        keys = self.memory_key(embedded)\n",
    "        values = self.memory_value(embedded)\n",
    "        return keys, values\n",
    "\n",
    "class ResidualSideNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualSideNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        residual = input_tensor\n",
    "        output = self.fc1(input_tensor)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output += residual  # Add residual connection\n",
    "        return output\n",
    "\n",
    "class MemoryRetrievalFusion(nn.Module):\n",
    "    def __init__(self, memory_dim, input_dim):\n",
    "        super(MemoryRetrievalFusion, self).__init__()\n",
    "\n",
    "        self.linear_query = nn.Linear(input_dim, memory_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, memory_augmented, transformer_outputs):\n",
    "        query = self.linear_query(transformer_outputs)\n",
    "        attention_scores = torch.matmul(query, memory_augmented.transpose(-1, -2))\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        fused_output = torch.matmul(attention_weights, memory_augmented)\n",
    "        return fused_output\n",
    "\n",
    "class BackboneLLM(nn.Module):\n",
    "    def __init__(self, model_name, config):\n",
    "        super(BackboneLLM, self).__init__()\n",
    "        self.gpt2 = GPT2Model.from_pretrained(model_name, config=config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        output = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        return output.last_hidden_state\n",
    "\n",
    "class LongMEM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, memory_dim, backbone_model_name, backbone_config):\n",
    "        super(LongMEM, self).__init__()\n",
    "\n",
    "        self.frozen_llm = BackboneLLM(backbone_model_name, backbone_config)\n",
    "        self.memory_bank = CachedMemoryBank(vocab_size, embedding_dim, memory_dim)\n",
    "        self.side_net = ResidualSideNet(embedding_dim, hidden_dim)\n",
    "        self.memory_fusion = MemoryRetrievalFusion(memory_dim, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Frozen LLM\n",
    "        with torch.no_grad():\n",
    "            frozen_last_hidden_state = self.frozen_llm(input_ids)\n",
    "\n",
    "        # Memory Bank\n",
    "        keys, values = self.memory_bank(input_ids)\n",
    "\n",
    "        # Transformer Outputs\n",
    "        transformer_outputs = frozen_last_hidden_state\n",
    "\n",
    "        # Memory Augmentation\n",
    "        memory_attention = torch.matmul(keys, transformer_outputs.transpose(-1, -2))\n",
    "        memory_attention = nn.functional.softmax(memory_attention, dim=-1)\n",
    "        memory_augmented = torch.matmul(memory_attention.transpose(-1, -2), values)\n",
    "\n",
    "        # Side Net\n",
    "        side_net_output = self.side_net(transformer_outputs)\n",
    "\n",
    "        # Memory Retrieval Fusion\n",
    "        fused_output = self.memory_fusion(memory_augmented, side_net_output)\n",
    "\n",
    "        # Final Linear Layer\n",
    "        logits = self.linear(fused_output)\n",
    "        return logits\n",
    "\n",
    "    def generate_text(self, input_ids, max_length):\n",
    "        with torch.no_grad():\n",
    "            output_ids = input_ids.clone()\n",
    "            for _ in range(max_length):\n",
    "                logits = self.forward(input_ids)\n",
    "                predicted_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                output_ids = torch.cat((output_ids, predicted_token.unsqueeze(1)), dim=1)\n",
    "                input_ids = output_ids\n",
    "\n",
    "        return output_ids\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 50257\n",
    "embedding_dim = 768\n",
    "hidden_dim = 768\n",
    "num_layers = 12\n",
    "memory_dim = 768\n",
    "backbone_model_name = \"gpt2\"  # Change this if you have a different pretrained model\n",
    "backbone_config = GPT2Config.from_pretrained(backbone_model_name)\n",
    "\n",
    "model = LongMEM(vocab_size, embedding_dim, hidden_dim, num_layers, memory_dim, backbone_model_name, backbone_config)\n",
    "\n",
    "# Example input\n",
    "input_ids = torch.tensor([[1, 2, 3, 4, 5]])\n",
    "\n",
    "# Forward pass\n",
    "output_logits = model(input_ids)\n",
    "\n",
    "print(\"Output logits shape:\", output_logits.shape)\n",
    "\n",
    "# Generate text\n",
    "max_length = 10\n",
    "generated_text = model.generate_text(input_ids, max_length)\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 50257\n",
    "embedding_dim = 768\n",
    "hidden_dim = 768\n",
    "num_layers = 12\n",
    "memory_dim = 768\n",
    "backbone_model_name = \"gpt2\"  # Change this if you have a different pretrained model\n",
    "backbone_config = GPT2Config.from_pretrained(backbone_model_name)\n",
    "\n",
    "model = LongMEM(vocab_size, embedding_dim, hidden_dim, num_layers, memory_dim, backbone_model_name, backbone_config)\n",
    "\n",
    "# Example input\n",
    "input_ids = torch.tensor([[1, 2, 3, 4, 5]])\n",
    "\n",
    "# Forward pass\n",
    "output_logits = model(input_ids)\n",
    "\n",
    "print(\"Output logits shape:\", output_logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPewJVGAplXx",
    "outputId": "6de9293a-51a3-4914-b4bf-837a12ddebf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_803387/1233479225.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits shape: torch.Size([1, 4, 50257])\n",
      "Keys shape: torch.Size([1, 4, 768])\n",
      "Values shape: torch.Size([1, 4, 768])\n",
      "Memory augmented shape: torch.Size([1, 4, 768])\n",
      "Generated text: tensor([[ 7454,  2402,   257,   640,  8610, 35641, 22542,  8610, 35641, 22686,\n",
      "         22686, 22686, 22686, 22686]])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text input\n",
    "text_input = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the text input\n",
    "input_ids = tokenizer.encode(text_input, return_tensors=\"pt\")\n",
    "\n",
    "# Create LONGMEM model\n",
    "longmem_model = LongMEM(vocab_size, embedding_dim, hidden_dim, num_layers, memory_dim, backbone_model_name, backbone_config)\n",
    "\n",
    "# Generate text using the model\n",
    "generated_ids = longmem_model.generate_text(input_ids, max_length=20)\n",
    "\n",
    "# Decode generated token IDs to text\n",
    "generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ai82goDiG6qy",
    "outputId": "253b6bcf-3718-4e10-bf38-8af9f2cd4654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/7.2 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m5.9/7.2 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F28TIPBuMja4",
    "outputId": "a9a96681-7bd3-4469-e02e-15ff4c1e20a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.0.234-py3-none-any.whl (1.3 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.2/1.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
      "Collecting langsmith<0.0.6,>=0.0.5 (from langchain)\n",
      "  Downloading langsmith-0.0.5-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langsmith, dataclasses-json, langchain\n",
      "Successfully installed dataclasses-json-0.5.9 langchain-0.0.234 langsmith-0.0.5 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTb7622-Gp28"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Instantiate the GPT2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-13b-delta-v1.1\")\n",
    "\n",
    "# Example text input\n",
    "text_input = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the text input\n",
    "input_ids = tokenizer.encode(text_input, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using the model\n",
    "max_length = 20\n",
    "generated_ids = model.generate_text(input_ids, max_length)\n",
    "\n",
    "# Decode generated token IDs to text\n",
    "generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LongMem with Customized LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SERVER HARDWARE SPECIFICATION:\n",
    "Model: Dell Precision 7920\n",
    "\n",
    "Processor: Intel Xeon Gold 5218 x 2  (64 Cores)\n",
    "\n",
    "Memory: 512 GB DDR4 3200 MHz\n",
    "\n",
    "SSD: Samsung 870 1 TB x 2 RAID 1\n",
    "\n",
    "GPU: Nvidia RTX 3090 x 2 (nvlink enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\r\n",
      "Built on Tue_May__3_18:49:52_PDT_2022\r\n",
      "Cuda compilation tools, release 11.7, V11.7.64\r\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 4.24.0\r\n",
      "Uninstalling protobuf-4.24.0:\r\n",
      "  Would remove:\r\n",
      "    /root/anaconda3/envs/py39/lib/python3.9/site-packages/google/_upb/_message.abi3.so\r\n",
      "    /root/anaconda3/envs/py39/lib/python3.9/site-packages/google/protobuf/*\r\n",
      "    /root/anaconda3/envs/py39/lib/python3.9/site-packages/protobuf-4.24.0.dist-info/*\r\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "!pip uninstall protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n",
      "Result of GPU operation:\n",
      "tensor([[-1.5003,  0.8487, -0.6878],\n",
      "        [-1.2165,  2.0938, -1.3832],\n",
      "        [ 0.2098, -1.6676,  1.0297]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set the CUDA_MPS_ACTIVE_THREAD_PERCENTAGE environment variable to 50%\n",
    "os.environ[\"CUDA_MPS_ACTIVE_THREAD_PERCENTAGE\"] = \"200\"\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    \n",
    "    # Select the first GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    # Perform some GPU operations\n",
    "    a = torch.randn(3, 3).to(device)\n",
    "    b = torch.randn(3, 3).to(device)\n",
    "    c = torch.matmul(a, b)\n",
    "    \n",
    "    print(\"Result of GPU operation:\")\n",
    "    print(c)\n",
    "else:\n",
    "    print(\"CUDA (GPU) is not available on this system.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.0.1+cu117 is compatible with CUDA version 11.7.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def is_cuda_compatible():\n",
    "    cuda_version = torch.version.cuda\n",
    "    required_cuda_versions = [\"10.1\", \"10.2\", \"11.1\",\"11.2\",\"11.3\",\"11.4\",\"11.5\",\"11.6\",\"11.7\"]  # Add the supported CUDA versions for your PyTorch version\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if cuda_version in required_cuda_versions:\n",
    "            print(f\"PyTorch version {torch.__version__} is compatible with CUDA version {cuda_version}.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"PyTorch version {torch.__version__} is not compatible with CUDA version {cuda_version}.\")\n",
    "            print(f\"Supported CUDA versions for PyTorch {torch.__version__}: {', '.join(required_cuda_versions)}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "        return False\n",
    "\n",
    "# Call the function to check compatibility\n",
    "is_cuda_compatible()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /root/anaconda3/envs/py39/lib/python3.9/site-packages (4.32.0.dev0)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers -i https://pypi.org/simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /root/anaconda3/envs/py39/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.6.3)\n",
      "Requirement already satisfied: wheel in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: cmake in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=d01200c69e43a9a69d8769517ef8d618a7bcfb68d7275dff36d27616306db25f\n",
      "  Stored in directory: /root/.cache/pip/wheels/f6/d8/b0/15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "Successfully built nvidia-ml-py3\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "GPU 0 Utilization: 0%, Memory Utilization: 0%\n",
      "GPU 1: NVIDIA GeForce RTX 3090\n",
      "GPU 1 Utilization: 0%, Memory Utilization: 0%\n"
     ]
    },
    {
     "ename": "NVMLError_Unknown",
     "evalue": "Unknown Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNVMLError_Unknown\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m device_count \u001b[38;5;241m=\u001b[39m pynvml\u001b[38;5;241m.\u001b[39mnvmlDeviceGetCount()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Get information about each GPU\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[43mpynvml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnvmlDeviceGetHandleByIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     gpu_info \u001b[38;5;241m=\u001b[39m pynvml\u001b[38;5;241m.\u001b[39mnvmlDeviceGetUtilizationRates(handle)\n\u001b[1;32m     13\u001b[0m     gpu_name \u001b[38;5;241m=\u001b[39m pynvml\u001b[38;5;241m.\u001b[39mnvmlDeviceGetName(handle)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pynvml/nvml.py:1655\u001b[0m, in \u001b[0;36mnvmlDeviceGetHandleByIndex\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m   1653\u001b[0m fn \u001b[38;5;241m=\u001b[39m _nvmlGetFunctionPointer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvmlDeviceGetHandleByIndex_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1654\u001b[0m ret \u001b[38;5;241m=\u001b[39m fn(c_index, byref(device))\n\u001b[0;32m-> 1655\u001b[0m \u001b[43m_nvmlCheckReturn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pynvml/nvml.py:765\u001b[0m, in \u001b[0;36m_nvmlCheckReturn\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_nvmlCheckReturn\u001b[39m(ret):\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (ret \u001b[38;5;241m!=\u001b[39m NVML_SUCCESS):\n\u001b[0;32m--> 765\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NVMLError(ret)\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mNVMLError_Unknown\u001b[0m: Unknown Error"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "\n",
    "# Initialize NVML\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# Get the number of available GPUs\n",
    "device_count = pynvml.nvmlDeviceGetCount()\n",
    "\n",
    "for i in range(device_count):\n",
    "    # Get information about each GPU\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    gpu_info = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "    gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "    \n",
    "    print(f\"GPU {i}: {gpu_name.decode('utf-8')}\")\n",
    "    print(f\"GPU {i} Utilization: {gpu_info.gpu}%, Memory Utilization: {gpu_info.memory}%\")\n",
    "    \n",
    "# Shutdown NVML\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to determine the device handle for GPU0000:D5:00.0: Unknown Error\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /root/anaconda3/envs/py39/lib/python3.9/site-packages (1.5.1)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code you provided implements a Long-term Memory Augmented Large Language Model (LLM) that uses a memory bank to cache long-form previous context or knowledge and further takes advantage of it via a decoupled memory module. The LLM model is designed to address the issue of memory staleness in existing large language models.\n",
    "\n",
    "Here's a summary of what the code is doing:\n",
    "\n",
    "Import the required libraries and modules.\n",
    "\n",
    "--Define a CachedMemoryBank class that serves as a memory bank to cache previous context or knowledge. It uses an embedding layer followed by linear layers to create keys and values representations from the input tensor.\n",
    "\n",
    "--Define a ResidualSideNet class that serves as a decoupled memory module. It takes the transformer outputs (contextual embeddings) as input, applies linear transformations with a ReLU activation function, and then adds a residual connection to the original input. This process allows the model to retrieve and fuse memory-augmented long-context information.\n",
    "\n",
    "--Define the main LLMWithMemory class, which is the Long-term Memory Augmented Large Language Model. This class combines the CachedMemoryBank and ResidualSideNet modules along with a transformer-based backbone language model (such as GPT-2). The LLMWithMemory class is responsible for memory augmentation and long-form language modeling.\n",
    "\n",
    "In the forward method of the LLMWithMemory class:\n",
    "a. The input tensor (input_ids) is passed through the memory bank to generate keys and values representations.\n",
    "b. The backbone language model is used to obtain transformer outputs (contextual embeddings) for the input tensor.\n",
    "c. The keys and transformer outputs are multiplied to get memory_attention, which is then softmaxed to obtain attention weights.\n",
    "d. The attention weights are used to retrieve memory_augmented representations from the memory bank using matrix multiplication.\n",
    "e. The transformer outputs are reshaped and passed through the ResidualSideNet to get side_net_output.\n",
    "f. The side_net_output is linearly transformed to predict logits for language modeling.\n",
    "\n",
    "The generate_text method is provided to generate text from the model. It takes an input tensor and performs a forward pass repeatedly, predicting the next token in a loop until the desired maximum length of the generated text is reached.\n",
    "\n",
    "The overall goal of this Long-term Memory Augmented Large Language Model is to allow the language model to remember and utilize long-form context or knowledge efficiently, thus improving its language modeling capabilities.\n",
    "\n",
    "Please note that the code provided may still require further adjustments or fine-tuning based on the specific context and downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.nn import TransformerDecoderLayer\n",
    "import math\n",
    "import torch.distributions as dist\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Tokenzier All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "\n",
    "nltk.download('inaugural')  # Download the Inaugural Address Corpus if you haven't already\n",
    "\n",
    "class CustomallTokenizer:\n",
    "    def __init__(self, text, max_vocab_size=5000, special_tokens=None):\n",
    "        self.text = text\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.special_tokens = special_tokens or ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.build_vocab()\n",
    "\n",
    "    def build_vocab(self):\n",
    "        word_counter = collections.Counter()\n",
    "        special_tokens = self.special_tokens\n",
    "        \n",
    "        words = self.text.split()  # Split the text into words\n",
    "        \n",
    "        word_counter.update(words)\n",
    "        \n",
    "        most_common = word_counter.most_common(self.max_vocab_size - len(special_tokens))\n",
    "        \n",
    "        self.vocab = {word: idx + len(special_tokens) for idx, (word, _) in enumerate(most_common)}\n",
    "        self.reverse_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "        \n",
    "        for idx, token in enumerate(special_tokens):\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        tokens = text.strip().split()\n",
    "        if add_special_tokens:\n",
    "            tokens = ['<START>'] + tokens + ['<END>']\n",
    "        token_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        tokens = [self.reverse_vocab.get(token_id, '<UNK>') for token_id in token_ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in ['<PAD>', '<START>', '<END>']]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def pad_sequences(self, sequences, max_length, padding_token='<PAD>'):\n",
    "        padded_sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) < max_length:\n",
    "                seq += [self.vocab.get(padding_token, self.vocab['<UNK>'])] * (max_length - len(seq))\n",
    "            padded_sequences.append(seq)\n",
    "        return padded_sequences\n",
    "    \n",
    "    def get_padding_token_id(self):\n",
    "        return self.vocab.get('<PAD>', self.vocab['<UNK>'])\n",
    "\n",
    "# Example usage\n",
    "# corpus_file = 'your_corpus.txt'\n",
    "# tokenizer = CustomTokenizer(corpus_file)\n",
    "\n",
    "# Get padding token ID\n",
    "# padding_token_id = tokenizer.get_padding_token_id()\n",
    "# print(\"Padding Token ID:\", padding_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, corpus_file, max_vocab_size=5000, special_tokens=None):\n",
    "        self.corpus_file = corpus_file\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.special_tokens = special_tokens or ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.build_vocab()\n",
    "\n",
    "    def build_vocab(self):\n",
    "        word_counter = collections.Counter()\n",
    "        special_tokens = self.special_tokens\n",
    "        \n",
    "        with open(self.corpus_file, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                words = line.strip().split()\n",
    "                word_counter.update(words)\n",
    "        \n",
    "        most_common = word_counter.most_common(self.max_vocab_size - len(special_tokens))\n",
    "        \n",
    "        self.vocab = {word: idx + len(special_tokens) for idx, (word, _) in enumerate(most_common)}\n",
    "        self.reverse_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "        \n",
    "        for idx, token in enumerate(special_tokens):\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        tokens = text.strip().split()\n",
    "        if add_special_tokens:\n",
    "            tokens = ['<START>'] + tokens + ['<END>']\n",
    "        token_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        tokens = [self.reverse_vocab.get(token_id, '<UNK>') for token_id in token_ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in ['<PAD>', '<START>', '<END>']]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def pad_sequences(self, sequences, max_length, padding_token='<PAD>'):\n",
    "        padded_sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) < max_length:\n",
    "                seq += [self.vocab.get(padding_token, self.vocab['<UNK>'])] * (max_length - len(seq))\n",
    "            padded_sequences.append(seq)\n",
    "        return padded_sequences\n",
    "\n",
    "# Example usage\n",
    "#corpus_file = 'your_corpus.txt'\n",
    "#tokenizer = CustomTokenizer(corpus_file)\n",
    "\n",
    "# Encoding\n",
    "#text = \"This is an example sentence.\"\n",
    "#encoded = tokenizer.encode(text)\n",
    "#print(\"Encoded:\", encoded)\n",
    "\n",
    "# Decoding\n",
    "#decoded = tokenizer.decode(encoded)\n",
    "#print(\"Decoded:\", decoded)\n",
    "\n",
    "# Padding sequences\n",
    "#sequences = [[1, 2, 3], [4, 5], [6, 7, 8, 9, 10]]\n",
    "#max_length = 6\n",
    "#padded_sequences = tokenizer.pad_sequences(sequences, max_length)\n",
    "#print(\"Padded Sequences:\", padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')  # Download the Gutenberg Corpus if you haven't already\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, corpus, max_vocab_size=5000, special_tokens=None):\n",
    "        self.corpus = corpus\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.special_tokens = special_tokens or ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.build_vocab()\n",
    "        self.vocab_size=0\n",
    "\n",
    "    def build_vocab(self):\n",
    "        word_counter = collections.Counter()  # Initialize a counter to count word occurrences.\n",
    "        special_tokens = self.special_tokens   # Get the list of special tokens.\n",
    "\n",
    "        # Loop through each file in the corpus to process words.\n",
    "        for file_id in self.corpus.fileids():\n",
    "            words = self.corpus.words(file_id)  # Get the words from the current file.\n",
    "            word_counter.update(words)           # Update the word counter with word occurrences.\n",
    "\n",
    "        # Get the most common words up to the specified vocabulary size.\n",
    "        most_common = word_counter.most_common(self.max_vocab_size - len(special_tokens))\n",
    "\n",
    "        # Build the vocabulary by assigning indices to words.\n",
    "        self.vocab = {word: idx + len(special_tokens) for idx, (word, _) in enumerate(most_common)}\n",
    "\n",
    "        # Build the reverse vocabulary for decoding.\n",
    "        self.reverse_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "                # Store word counts for each word\n",
    "        self.word_counts = dict(word_counter)\n",
    "\n",
    "        # Assign indices to special tokens in the vocabulary.\n",
    "        for idx, token in enumerate(special_tokens):\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token  # Comment explaining this part.\n",
    "        self.vocab_size= len(self.vocab)\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        # Split the input text into individual tokens.\n",
    "        tokens = text.strip().split()\n",
    "    \n",
    "        # Add special tokens at the beginning and end if requested.\n",
    "        if add_special_tokens:\n",
    "            tokens = ['<START>'] + tokens + ['<END>']\n",
    "    \n",
    "        # Convert tokens to their corresponding IDs using the vocabulary.\n",
    "        token_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "    \n",
    "        # Return the list of token IDs.\n",
    "        return token_ids  # Comment explaining this code\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        # Get corresponding tokens from token IDs using the reverse vocabulary.\n",
    "        tokens = [self.reverse_vocab.get(token_id, '<UNK>') for token_id in token_ids]\n",
    "    \n",
    "        # If requested, skip specified special tokens during decoding.\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in ['<PAD>', '<START>', '<END>']]\n",
    "    \n",
    "        # Join the tokens to form the decoded text.\n",
    "        return ' '.join(tokens)  # Comment explaining this code\n",
    "        \n",
    "    def pad_sequences(self, sequences, max_length, padding_token='<PAD>'):\n",
    "        padded_sequences = []  # Initialize a list to store padded sequences.\n",
    "        \n",
    "        # Loop through each sequence.\n",
    "        for seq in sequences:\n",
    "            # Check if the sequence length is less than the desired maximum length.\n",
    "            if len(seq) < max_length:\n",
    "                # Calculate the number of padding tokens needed.\n",
    "                padding_tokens_needed = max_length - len(seq)\n",
    "                \n",
    "                # Add padding tokens to the sequence to match the maximum length.\n",
    "                seq += [self.vocab.get(padding_token, self.vocab['<UNK>'])] * padding_tokens_needed\n",
    "            \n",
    "            # Append the padded sequence to the list.\n",
    "            padded_sequences.append(seq)\n",
    "        \n",
    "        # Return the list of padded sequences.\n",
    "        return padded_sequences  # Comment explaining this code\n",
    "    \n",
    "    def get_padding_token_id(self):\n",
    "        # Retrieve the ID of the padding token from the vocabulary.\n",
    "        return self.vocab.get('<PAD>', self.vocab['<UNK>'])  # Comment explaining this code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 51160\n",
      "Padding Token ID: 0\n",
      "Encoded: [2, 261, 25, 76, 2704, 2846, 7, 3]\n",
      "Decoded: This is an example sentence .\n",
      "Padded Sequences: [[1, 2, 3, 0, 0, 0], [4, 5, 0, 0, 0, 0], [6, 7, 8, 9, 10, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from nltk.corpus import gutenberg\n",
    "import pickle\n",
    "\n",
    "tokenizer = CustomTokenizer(gutenberg,235892)\n",
    "# Print the vocabulary size of the tokenizer\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "# Get the ID of the padding token\n",
    "padding_token_id = tokenizer.get_padding_token_id()\n",
    "print(\"Padding Token ID:\", padding_token_id)\n",
    "\n",
    "# Encoding\n",
    "text = \"This is an example sentence .\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# Decoding\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "# Padding sequences\n",
    "sequences = [[1, 2, 3], [4, 5], [6, 7, 8, 9, 10]]\n",
    "max_length = 6\n",
    "padded_sequences = tokenizer.pad_sequences(sequences, max_length)\n",
    "print(\"Padded Sequences:\", padded_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved!\n",
      "Tokenizer saved!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fileids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer saved!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m loaded_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Now you can use the loaded_tokenizer for encoding, decoding, etc.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 19\u001b[0m, in \u001b[0;36mload_tokenizer\u001b[0;34m(load_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(load_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     tokenizer_info \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Initialize with None as the corpus\u001b[39;00m\n\u001b[1;32m     20\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mword_counts \u001b[38;5;241m=\u001b[39m tokenizer_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_counts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mmax_vocab_size \u001b[38;5;241m=\u001b[39m tokenizer_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_vocab_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m, in \u001b[0;36mCustomTokenizer.__init__\u001b[0;34m(self, corpus, max_vocab_size, special_tokens)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreverse_vocab \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mCustomTokenizer.build_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens   \u001b[38;5;66;03m# Get the list of special tokens.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Loop through each file in the corpus to process words.\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m():\n\u001b[1;32m     22\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mwords(file_id)  \u001b[38;5;66;03m# Get the words from the current file.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     word_counter\u001b[38;5;241m.\u001b[39mupdate(words)           \u001b[38;5;66;03m# Update the word counter with word occurrences.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fileids'"
     ]
    }
   ],
   "source": [
    "def save_tokenizer(tokenizer, save_path):\n",
    "    tokenizer_info = {\n",
    "        'word_counts': tokenizer.word_counts,\n",
    "        'max_vocab_size': tokenizer.max_vocab_size,\n",
    "        'special_tokens': tokenizer.special_tokens,\n",
    "        'vocab': tokenizer.vocab,\n",
    "        'reverse_vocab': tokenizer.reverse_vocab,\n",
    "    }\n",
    "\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer_info, f)\n",
    "\n",
    "    print(\"Tokenizer saved!\")\n",
    "\n",
    "def load_tokenizer(load_path):\n",
    "    with open(load_path, 'rb') as f:\n",
    "        tokenizer_info = pickle.load(f)\n",
    "\n",
    "    tokenizer = CustomTokenizer(None)  # Initialize with None as the corpus\n",
    "    tokenizer.word_counts = tokenizer_info['word_counts']\n",
    "    tokenizer.max_vocab_size = tokenizer_info['max_vocab_size']\n",
    "    tokenizer.special_tokens = tokenizer_info['special_tokens']\n",
    "    tokenizer.vocab = tokenizer_info['vocab']\n",
    "    tokenizer.reverse_vocab = tokenizer_info['reverse_vocab']\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "# Example usage\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = CustomTokenizer(gutenberg, max_vocab_size=235892)\n",
    "\n",
    "# Save the tokenizer\n",
    "save_path = \"tokenizer.pkl\"\n",
    "save_tokenizer(tokenizer, save_path)\n",
    "print(\"Tokenizer saved!\")\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = load_tokenizer(save_path)\n",
    "print(\"Tokenizer loaded!\")\n",
    "\n",
    "# Now you can use the loaded_tokenizer for encoding, decoding, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the CustomTokenizer class and corpus text as shown before\n",
    "# Instantiate the CustomTokenizer\n",
    "tokenizer = CustomTokenizer(combined_text)\n",
    "\n",
    "# Tokenize the corpus text\n",
    "tokenized_text = tokenizer.encode(combined_text, add_special_tokens=False)\n",
    "\n",
    "# Create input-output pairs for LM training\n",
    "seq_length = 50  # Adjust this to your desired sequence length\n",
    "input_sequences = [tokenized_text[i : i + seq_length] for i in range(0, len(tokenized_text) - seq_length)]\n",
    "output_sequences = [tokenized_text[i + 1 : i + seq_length + 1] for i in range(0, len(tokenized_text) - seq_length)]\n",
    "\n",
    "# Transform sequences using CustomTokenizer\n",
    "encoded_input_sequences = [tokenizer.encode(\" \".join(tokens), add_special_tokens=False) for tokens in input_sequences]\n",
    "encoded_output_sequences = [tokenizer.encode(\" \".join(tokens), add_special_tokens=False) for tokens in output_sequences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize Large Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, feedforward_size, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Multi-head self-attention layer\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout_rate)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Position-wise feedforward network\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, feedforward_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feedforward_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        # Multi-head self-attention\n",
    "        # Compute multi-head self-attention using the provided inputs\n",
    "        attention_output, _ = self.self_attention(inputs, inputs, inputs, attn_mask=attention_mask)\n",
    "        # Apply dropout to the attention output and add it to the original inputs\n",
    "        attention_output = self.dropout(attention_output) + inputs\n",
    "        # Apply layer normalization to the attention output\n",
    "        attention_output = self.norm1(attention_output)\n",
    "        \n",
    "        # Position-wise feedforward\n",
    "        # Pass the attention output through the position-wise feedforward network\n",
    "        ff_output = self.feedforward(attention_output)\n",
    "        # Apply dropout to the feedforward output and add it to the attention output\n",
    "        ff_output = self.dropout(ff_output) + attention_output\n",
    "        # Apply layer normalization to the feedforward output\n",
    "        ff_output = self.norm2(ff_output)\n",
    "        \n",
    "        return ff_output\n",
    "\n",
    "class LargeLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_heads, feedforward_size, dropout_rate=0.1, max_sequence_length=512):\n",
    "        super(LargeLanguageModel, self).__init__()\n",
    "        \n",
    "        # Set the maximum sequence length and vocabulary size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Create the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Create a list of transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_dim, num_heads, feedforward_size, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Create the final linear layer for prediction\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Embed the input tokens\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # Apply each transformer block to the embedded input\n",
    "        transformer_output = embedded\n",
    "        for block in self.transformer_blocks:\n",
    "            transformer_output = block(transformer_output, attention_mask=attention_mask)\n",
    "        \n",
    "        # Reshape the transformer_output for the linear layer\n",
    "        batch_size, seq_length, hidden_dim = transformer_output.size()\n",
    "        transformer_output = transformer_output.view(batch_size * seq_length, hidden_dim)\n",
    "        \n",
    "        # Pass the transformer_output through the linear layer for prediction\n",
    "        logits = self.linear(transformer_output)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate_text(self, input_ids, max_length, temperature=1.0, top_k=None, top_p=None):\n",
    "        # Clone the input_ids to avoid modifying the original\n",
    "        generated_ids = input_ids.clone()\n",
    "    \n",
    "        # Loop to generate text up to max_length\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass to get logits for the next token\n",
    "            logits = self.forward(generated_ids)\n",
    "    \n",
    "            # Apply temperature for token sampling\n",
    "            logits = logits[-1, :] / temperature\n",
    "    \n",
    "            # Sampling logic based on top_k and top_p\n",
    "            if top_k is not None:\n",
    "                # Apply top-k sampling\n",
    "                logits, indices = torch.topk(logits, top_k)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                predicted_id = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "            elif top_p is not None:\n",
    "                # Apply nucleus (top-p) sampling\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1]\n",
    "                sorted_indices_to_remove[:, 0] = 0\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = float('-inf')\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                predicted_id = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "            else:\n",
    "                # Regular softmax-based sampling\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                predicted_id = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "    \n",
    "    \n",
    "            # Append the predicted_id to generated_ids\n",
    "            generated_ids = torch.cat((generated_ids, predicted_id.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "    \n",
    "            # Check if the generated token is the end token\n",
    "            if predicted_id == self.vocab_size - 1:\n",
    "                break\n",
    "    \n",
    "        return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gutenberg\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTokenizer\u001b[49m(gutenberg,\u001b[38;5;241m235892\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Print the vocabulary size of the tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mvocab)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "tokenizer = CustomTokenizer(gutenberg,235892)\n",
    "# Print the vocabulary size of the tokenizer\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Encoding\n",
    "text = \"This is an example sentence .\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "# Decoding\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "# Padding sequences\n",
    "sequences = [[1, 2, 3], [4, 5], [6, 7, 8, 9, 10]]\n",
    "max_length = 6\n",
    "padded_sequences = tokenizer.pad_sequences(sequences, max_length)\n",
    "print(\"Padded Sequences:\", padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwardtransformer_output.shape torch.Size([1, 7, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([7, 768])\n",
      "torch.Size([7, 51160])\n",
      "forwardtransformer_output.shape torch.Size([1, 7, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([7, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 8, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([8, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 9, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([9, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 10, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([10, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 11, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([11, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 12, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([12, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 13, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([13, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 14, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([14, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 15, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([15, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 16, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([16, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 17, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([17, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 18, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([18, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 19, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([19, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 20, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([20, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 21, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([21, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 22, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([22, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 23, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([23, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 24, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([24, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 25, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([25, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 26, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([26, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 27, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([27, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 28, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([28, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 29, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([29, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 30, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([30, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 31, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([31, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 32, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([32, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 33, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([33, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 34, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([34, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 35, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([35, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 36, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([36, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 37, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([37, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 38, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([38, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 39, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([39, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 40, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([40, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 41, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([41, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 42, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([42, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 43, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([43, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 44, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([44, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 45, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([45, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 46, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([46, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 47, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([47, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 48, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([48, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 49, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([49, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 50, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([50, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 51, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([51, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 52, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([52, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 53, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([53, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 54, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([54, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 55, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([55, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 56, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([56, 768])\n",
      "Generated Text: tensor([[    1,     2,     3,     4,     5,     6,     7, 19469, 32573,  2163,\n",
      "          8045,  4396, 14158,  4454, 23369, 15637, 48957, 32521, 23982, 13748,\n",
      "          5080, 48158, 21860, 10415, 49770, 20667, 10558, 27626, 45211,  1375,\n",
      "         43082, 26076,  2008,  2308, 11177, 22491, 31531, 31158, 34407,  3249,\n",
      "         27941, 47555,  3402, 41400, 12173, 12444, 20986, 12421, 23366, 17558,\n",
      "         31286, 30372, 41204, 38208, 38325, 15318, 11488]])\n",
      "torch.Size([1, 57])\n",
      "Decoded: <UNK>\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "vocab_size = vocab_size\n",
    "embedding_dim = 768\n",
    "hidden_dim = 768\n",
    "num_layers = 40\n",
    "num_heads = 12\n",
    "feedforward_size = 4*hidden_dim\n",
    "dropout_rate = 0.1\n",
    "model = LargeLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, num_heads, feedforward_size, dropout_rate)\n",
    "input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7]])\n",
    "logits = model(input_ids)\n",
    "print(logits.shape)\n",
    "\n",
    "# Generate text\n",
    "generated_text = model.generate_text(input_ids, max_length=50, temperature=0.8)\n",
    "#decoded_text = tokenizer.decode(generated_text)\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(generated_text.shape)\n",
    "# Decoding\n",
    "decoded = tokenizer.decode(generated_text)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LongMem Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedMemoryBank(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, memory_dim):\n",
    "        super(CachedMemoryBank, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.memory_key = nn.Linear(embedding_dim, memory_dim)\n",
    "        self.memory_value = nn.Linear(embedding_dim, memory_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        keys = self.memory_key(embedded)\n",
    "        values = self.memory_value(embedded)\n",
    "        return keys, values\n",
    "\n",
    "class ResidualSideNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualSideNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        residual = input_tensor  # Store the input tensor for the residual connection\n",
    "        output = self.fc1(input_tensor)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output += residual  # Add residual connection\n",
    "        return output\n",
    "\n",
    "class MemoryAugmentation(nn.Module):\n",
    "    def __init__(self, memory_dim):\n",
    "        super(MemoryAugmentation, self).__init__()\n",
    "        self.memory_dim = memory_dim\n",
    "\n",
    "    def forward(self, keys, values, transformer_outputs):\n",
    "        transformer_outputs = transformer_outputs[0]\n",
    "\n",
    "        # Convert keys and values to float16 (matching the model's requirement)\n",
    "        keys = keys.float()\n",
    "        values = values.float()\n",
    "\n",
    "        # Memory augmentation\n",
    "        memory_attention = torch.matmul(keys, transformer_outputs.transpose(-1, 0))\n",
    "        memory_attention = memory_attention.transpose(-1, -2)\n",
    "\n",
    "        # Apply the memory-augmented input to the SideNet\n",
    "        memory_augmented = torch.matmul(memory_attention, values)\n",
    "        transformer_outputs = transformer_outputs + memory_augmented\n",
    "        return memory_augmented\n",
    "\n",
    "class MemoryRetrievalFusion(nn.Module):\n",
    "    def __init__(self, memory_dim):\n",
    "        super(MemoryRetrievalFusion, self).__init__()\n",
    "\n",
    "        self.linear_query = nn.Linear(memory_dim, memory_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, memory_augmented, transformer_outputs):\n",
    "        batch_size, seq_length, _ = transformer_outputs.size()  # Get batch_size and seq_length\n",
    "        query = self.linear_query(transformer_outputs.view(batch_size * seq_length, -1))\n",
    "        attention_scores = torch.matmul(query, memory_augmented.transpose(-1, -2))\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        fused_output = torch.matmul(attention_weights, memory_augmented)\n",
    "        return fused_output\n",
    "\n",
    "\n",
    "\n",
    "class BackboneLLM(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BackboneLLM, self).__init__()\n",
    "        self.Llama = LlamaForCausalLM.from_pretrained(model_name,\n",
    "                                            load_in_8bit=True,\n",
    "                                            device_map='auto',\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            offload_folder = 'model')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        output = self.Llama(input_ids, attention_mask=attention_mask)\n",
    "        return output.logits\n",
    "\n",
    "class LongMEM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, memory_dim, feedforward_size, dropout_rate):\n",
    "        super(LongMEM, self).__init__()\n",
    "        input_dim=vocab_size\n",
    "        self.frozen_llm = LargeLanguageModel(vocab_size,embedding_dim, hidden_dim, num_layers, num_heads, feedforward_size, dropout_rate)\n",
    "        self.memory_bank = CachedMemoryBank(vocab_size, hidden_dim, memory_dim)\n",
    "        self.side_net = ResidualSideNet(memory_dim,hidden_dim)\n",
    "        self.memory_augmentation = MemoryAugmentation(memory_dim)\n",
    "        self.memory_fusion = MemoryRetrievalFusion(memory_dim)\n",
    "        #linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "    def forward(self, input_ids,attention_mask=None):\n",
    "        # Generate attention mask (corrected shape)\n",
    "        #if attention_mask is None:\n",
    "        #    batch_size, seq_length = input_ids.size()\n",
    "            # Generate causal attention mask\n",
    "        #    causal_mask = torch.triu(torch.ones((seq_length, seq_length), dtype=torch.bool), diagonal=1)\n",
    "        #    causal_mask = causal_mask.unsqueeze(0)  # Add batch dimension\n",
    "        #    causal_mask = causal_mask.unsqueeze(1).expand(batch_size, 12, seq_length, seq_length)  # Expand for num_heads\n",
    "            # Invert the mask: 0s become True (mask) and 1s become False (no mask)\n",
    "        #    attention_mask = ~causal_mask\n",
    "\n",
    "        \n",
    "        # Ensure attention_mask has the correct shape (batch_size, seq_length)\n",
    "        #attention_mask = attention_mask[:, :input_ids.size(1)] \n",
    "\n",
    "        transformer_outputs = self.frozen_llm(input_ids)\n",
    "        #print(\"transformer_outputs.shape:\",transformer_outputs.shape)\n",
    "        # Assuming transformer_outputs has the shape torch.Size([7, 50257])\n",
    "        transformer_outputs = transformer_outputs.unsqueeze(0)  # Add a new dimension at the beginning\n",
    "        #print(\"transformer_outputs shape:\", transformer_outputs.shape)  # torch.Size([1, 7, 50257])\n",
    "        keys, values =self.memory_bank(input_ids)\n",
    "        memory_augmented = self.memory_augmentation(keys, values, transformer_outputs)\n",
    "        #print(\"memory_augmented shape:\", memory_augmented.shape)\n",
    "        #input_tensor=transformer_outputs.view(-1, transformer_outputs.size(-1))\n",
    "        side_net_output = self.side_net(transformer_outputs)\n",
    "        #print(\"side_net_output shape:\", side_net_output.shape)\n",
    "        #side_net_output =side_net_output.view(-1, side_net_output.size(-1))\n",
    "        fused_output = self.memory_fusion(memory_augmented, side_net_output)\n",
    "        # Reshape transformer_output before passing to linear layer\n",
    "        batch_size, seq_length, hidden_dim = fused_output.size()\n",
    "        fused_output = fused_output.view(batch_size * seq_length,hidden_dim)\n",
    "        #print(\"forwardtransformer_outputnew.shape\",fused_output.shape)\n",
    "        #logits = self.frozen_llm.linear(fused_output)\n",
    "        logits = torch.matmul(fused_output, self.frozen_llm.linear.weight)\n",
    "        return logits,memory_augmented\n",
    "\n",
    "    def generate_text(self, input_ids, max_length):\n",
    "        with torch.no_grad():\n",
    "            output_ids = input_ids.clone()\n",
    "            for _ in range(max_length):\n",
    "                logits = model.forward(input_ids)\n",
    "                #print(\"logits.shape\",logits.shape)\n",
    "                logits=logits.unsqueeze(0)\n",
    "                predicted_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                #print(\"predicted_token.shape\",predicted_token.shape)\n",
    "                predicted_token = predicted_token.unsqueeze(0)  # Unsqueeze along the batch dimension\n",
    "                #print(\"output_ids.shape\",output_ids.shape)\n",
    "                #print(\"predicted_token.shape\",predicted_token.shape)\n",
    "                output_ids= torch.cat((output_ids, predicted_token), dim=1)\n",
    "                input_ids = predicted_token  # Update input_ids for the next iteration\n",
    "        return output_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LongMem with GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class CachedMemoryBank(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, memory_dim):\n",
    "        super(CachedMemoryBank, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.memory_key = nn.Linear(embedding_dim, memory_dim)\n",
    "        self.memory_value = nn.Linear(embedding_dim, memory_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        keys = self.memory_key(embedded)\n",
    "        values = self.memory_value(embedded)\n",
    "        return keys, values\n",
    "\n",
    "class ResidualSideNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualSideNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        residual = input_tensor  # Store the input tensor for the residual connection\n",
    "        output = self.fc1(input_tensor)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output += residual  # Add residual connection\n",
    "        return output\n",
    "\n",
    "class MemoryAugmentation(nn.Module):\n",
    "    def __init__(self, memory_dim):\n",
    "        super(MemoryAugmentation, self).__init__()\n",
    "        self.memory_dim = memory_dim\n",
    "\n",
    "    def forward(self, keys, values, transformer_outputs):\n",
    "        transformer_outputs = transformer_outputs[0]\n",
    "\n",
    "        # Convert keys and values to float16 (matching the model's requirement)\n",
    "        keys = keys.float()\n",
    "        values = values.float()\n",
    "\n",
    "        # Memory augmentation\n",
    "        memory_attention = torch.matmul(keys, transformer_outputs.transpose(-1, 0))\n",
    "        memory_attention = memory_attention.transpose(-1, -2)\n",
    "\n",
    "        # Apply the memory-augmented input to the SideNet\n",
    "        memory_augmented = torch.matmul(memory_attention, values)\n",
    "        transformer_outputs = transformer_outputs + memory_augmented\n",
    "        return memory_augmented\n",
    "\n",
    "class MemoryRetrievalFusion(nn.Module):\n",
    "    def __init__(self, memory_dim):\n",
    "        super(MemoryRetrievalFusion, self).__init__()\n",
    "\n",
    "        self.linear_query = nn.Linear(memory_dim, memory_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, memory_augmented, transformer_outputs):\n",
    "        batch_size, seq_length, _ = transformer_outputs.size()  # Get batch_size and seq_length\n",
    "        query = self.linear_query(transformer_outputs.view(batch_size * seq_length, -1))\n",
    "        attention_scores = torch.matmul(query, memory_augmented.transpose(-1, -2))\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        fused_output = torch.matmul(attention_weights, memory_augmented)\n",
    "        return fused_output\n",
    "\n",
    "\n",
    "\n",
    "class BackboneLLM(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BackboneLLM, self).__init__()\n",
    "        self.Llama = LlamaForCausalLM.from_pretrained(model_name,\n",
    "                                            load_in_8bit=True,\n",
    "                                            device_map='auto',\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            offload_folder = 'model')\n",
    "        self.linear=self.Llama.lm_head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        output = self.Llama(input_ids, attention_mask=attention_mask)\n",
    "        return output.logits\n",
    "\n",
    "class LongMEMwithPretrained(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, memory_dim,backbone_model_name):\n",
    "        super(LongMEMwithPretrained, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.frozen_llm = AutoModelForCausalLM.from_pretrained(backbone_model_name)\n",
    "\n",
    "        #self.frozen_llm = LargeLanguageModel(vocab_size,embedding_dim, hidden_dim, num_layers, num_heads, feedforward_size, dropout_rate)\n",
    "        self.memory_bank = CachedMemoryBank(vocab_size, hidden_dim, memory_dim)\n",
    "        self.side_net = ResidualSideNet(memory_dim,hidden_dim)\n",
    "        self.memory_augmentation = MemoryAugmentation(memory_dim)\n",
    "        self.memory_fusion = MemoryRetrievalFusion(memory_dim)\n",
    "        #linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "    def forward(self, input_ids,attention_mask=None):\n",
    "        # Generate attention mask (corrected shape)\n",
    "        #if attention_mask is None:\n",
    "        #    batch_size, seq_length = input_ids.size()\n",
    "            # Generate causal attention mask\n",
    "        #    causal_mask = torch.triu(torch.ones((seq_length, seq_length), dtype=torch.bool), diagonal=1)\n",
    "        #    causal_mask = causal_mask.unsqueeze(0)  # Add batch dimension\n",
    "        #    causal_mask = causal_mask.unsqueeze(1).expand(batch_size, 12, seq_length, seq_length)  # Expand for num_heads\n",
    "            # Invert the mask: 0s become True (mask) and 1s become False (no mask)\n",
    "        #    attention_mask = ~causal_mask\n",
    "\n",
    "        \n",
    "        # Ensure attention_mask has the correct shape (batch_size, seq_length)\n",
    "        #attention_mask = attention_mask[:, :input_ids.size(1)] \n",
    "\n",
    "        transformer_outputs = self.frozen_llm(input_ids)[0]\n",
    "        #print(\"transformer_outputs.shape:\",transformer_outputs.shape)\n",
    "        # Assuming transformer_outputs has the shape torch.Size([7, 50257])\n",
    "        #transformer_outputs = transformer_outputs.unsqueeze(0)  # Add a new dimension at the beginning\n",
    "        #print(\"transformer_outputs shape:\", transformer_outputs.shape)  # torch.Size([1, 7, 50257])\n",
    "        keys, values =self.memory_bank(input_ids)\n",
    "        # Convert keys and values to float16 (matching the model's requirement)\n",
    "        keys = keys.float()\n",
    "        values = values.float()\n",
    "        memory_augmented = self.memory_augmentation(keys, values, transformer_outputs)\n",
    "        #print(\"memory_augmented shape:\", memory_augmented.shape)\n",
    "        #input_tensor=transformer_outputs.view(-1, transformer_outputs.size(-1))\n",
    "        side_net_output = self.side_net(transformer_outputs)\n",
    "        #print(\"side_net_output shape:\", side_net_output.shape)\n",
    "        #side_net_output =side_net_output.view(-1, side_net_output.size(-1))\n",
    "        fused_output = self.memory_fusion(memory_augmented, side_net_output)\n",
    "        # Reshape transformer_output before passing to linear layer\n",
    "        batch_size, seq_length, hidden_dim = fused_output.size()\n",
    "        fused_output = fused_output.view(batch_size * seq_length, hidden_dim)\n",
    "        print(\"forwardtransformer_outputnew.shape\",fused_output.shape)\n",
    "        print(\"self.frozen_llm.lm_head.weight\",self.frozen_llm.lm_head.weight.shape)\n",
    "        #logits = self.frozen_llm.lm_head(fused_output.view(1024, self.vocab_size, -1))\n",
    "        logits = torch.matmul(fused_output.view(1024, self.vocab_size), self.frozen_llm.linear.weight)\n",
    "        return logits,memory_augmented\n",
    "\n",
    "    def generate_text(self, input_ids, max_length):\n",
    "        with torch.no_grad():\n",
    "            output_ids = input_ids.clone()\n",
    "            for _ in range(max_length):\n",
    "                logits = model.forward(input_ids)\n",
    "                #print(\"logits.shape\",logits.shape)\n",
    "                logits=logits.unsqueeze(0)\n",
    "                predicted_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                #print(\"predicted_token.shape\",predicted_token.shape)\n",
    "                predicted_token = predicted_token.unsqueeze(0)  # Unsqueeze along the batch dimension\n",
    "                #print(\"output_ids.shape\",output_ids.shape)\n",
    "                #print(\"predicted_token.shape\",predicted_token.shape)\n",
    "                output_ids= torch.cat((output_ids, predicted_token), dim=1)\n",
    "                input_ids = predicted_token  # Update input_ids for the next iteration\n",
    "        return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(backbone_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=50257, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-31 14:31:38,121] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 14:31:41.884234: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-31 14:31:41.929225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 14:31:42.766325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import DataParallel\n",
    "print(torch.cuda.is_available())\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "backbone_model_name=\"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(backbone_model_name)\n",
    "# Example usage\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 768\n",
    "hidden_dim = 768\n",
    "num_layers = 12\n",
    "memory_dim = tokenizer.vocab_size\n",
    "#backbone_config = GPT2Config.from_pretrained(backbone_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LongMEMwithPretrained(vocab_size, embedding_dim, hidden_dim, memory_dim,backbone_model_name).to(device)\n",
    "\n",
    "# Encode a text sequence to obtain input_ids\n",
    "text = \"This is a sample input sentence.\"\n",
    "# Tokenize the input text and convert to input_ids\n",
    "tokens = tokenizer.encode(text)\n",
    "#input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(\"tokens:\")\n",
    "print(tokens)\n",
    "print(\"Input IDs:\")\n",
    "print(input_ids)\n",
    "print(\"Input IDs shape:\")\n",
    "print(input_ids.shape)\n",
    "\n",
    "# Forward pass\n",
    "output_logits = model(input_ids.to(device))\n",
    "\n",
    "print(\"Output logits shape:\", output_logits.shape)\n",
    "\n",
    "# Generate text\n",
    "max_length = 20\n",
    "generated_text = model.generate_text(input_ids, max_length)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LongMem with Vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-31 11:11:02,038] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 11:11:05.723781: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-31 11:11:05.769254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 11:11:06.594894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "class CachedMemoryBank(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, memory_dim):\n",
    "        super(CachedMemoryBank, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.memory_key = nn.Linear(embedding_dim, memory_dim)\n",
    "        self.memory_value = nn.Linear(embedding_dim, memory_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        keys = self.memory_key(embedded)\n",
    "        values = self.memory_value(embedded)\n",
    "        return keys, values\n",
    "\n",
    "class ResidualSideNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ResidualSideNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        residual = input_tensor  # Store the input tensor for the residual connection\n",
    "        output = self.fc1(input_tensor)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output += residual  # Add residual connection\n",
    "        return output\n",
    "\n",
    "class MemoryAugmentation(nn.Module):\n",
    "    def __init__(self, memory_dim):\n",
    "        super(MemoryAugmentation, self).__init__()\n",
    "        self.memory_dim = memory_dim\n",
    "\n",
    "    def forward(self, keys, values, transformer_outputs):\n",
    "        transformer_outputs = transformer_outputs[0]\n",
    "\n",
    "        # Convert keys and values to float16 (matching the model's requirement)\n",
    "        keys = keys.float()\n",
    "        values = values.float()\n",
    "\n",
    "        # Memory augmentation\n",
    "        memory_attention = torch.matmul(keys, transformer_outputs.transpose(-1, 0))\n",
    "        memory_attention = memory_attention.transpose(-1, -2)\n",
    "\n",
    "        # Apply the memory-augmented input to the SideNet\n",
    "        memory_augmented = torch.matmul(memory_attention, values)\n",
    "        transformer_outputs = transformer_outputs + memory_augmented\n",
    "        return memory_augmented\n",
    "\n",
    "class MemoryRetrievalFusion(nn.Module):\n",
    "    def __init__(self, memory_dim):\n",
    "        super(MemoryRetrievalFusion, self).__init__()\n",
    "\n",
    "        self.linear_query = nn.Linear(memory_dim, memory_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, memory_augmented, transformer_outputs):\n",
    "        batch_size, seq_length, _ = transformer_outputs.size()  # Get batch_size and seq_length\n",
    "        query = self.linear_query(transformer_outputs.view(batch_size * seq_length, -1))\n",
    "        attention_scores = torch.matmul(query, memory_augmented.transpose(-1, -2))\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        fused_output = torch.matmul(attention_weights, memory_augmented)\n",
    "        return fused_output\n",
    "\n",
    "\n",
    "\n",
    "class BackboneLLM(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BackboneLLM, self).__init__()\n",
    "        self.Llama = LlamaForCausalLM.from_pretrained(model_name,\n",
    "                                            load_in_8bit=True,\n",
    "                                            device_map='auto',\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            offload_folder = 'model')\n",
    "        self.linear=self.Llama.lm_head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        output = self.Llama(input_ids, attention_mask=attention_mask)\n",
    "        return output.logits\n",
    "\n",
    "class LongMEMwithPretrained(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, memory_dim,backbone_model_name):\n",
    "        super(LongMEMwithPretrained, self).__init__()\n",
    "        input_dim=vocab_size\n",
    "        self.frozen_llm = BackboneLLM(backbone_model_name)\n",
    "        #self.frozen_llm = LargeLanguageModel(vocab_size,embedding_dim, hidden_dim, num_layers, num_heads, feedforward_size, dropout_rate)\n",
    "        self.memory_bank = CachedMemoryBank(vocab_size, hidden_dim, memory_dim)\n",
    "        self.side_net = ResidualSideNet(memory_dim,hidden_dim)\n",
    "        self.memory_augmentation = MemoryAugmentation(memory_dim)\n",
    "        self.memory_fusion = MemoryRetrievalFusion(memory_dim)\n",
    "        #linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "    def forward(self, input_ids,attention_mask=None):\n",
    "        # Generate attention mask (corrected shape)\n",
    "        #if attention_mask is None:\n",
    "        #    batch_size, seq_length = input_ids.size()\n",
    "            # Generate causal attention mask\n",
    "        #    causal_mask = torch.triu(torch.ones((seq_length, seq_length), dtype=torch.bool), diagonal=1)\n",
    "        #    causal_mask = causal_mask.unsqueeze(0)  # Add batch dimension\n",
    "        #    causal_mask = causal_mask.unsqueeze(1).expand(batch_size, 12, seq_length, seq_length)  # Expand for num_heads\n",
    "            # Invert the mask: 0s become True (mask) and 1s become False (no mask)\n",
    "        #    attention_mask = ~causal_mask\n",
    "\n",
    "        \n",
    "        # Ensure attention_mask has the correct shape (batch_size, seq_length)\n",
    "        #attention_mask = attention_mask[:, :input_ids.size(1)] \n",
    "\n",
    "        transformer_outputs = self.frozen_llm(input_ids)\n",
    "        #print(\"transformer_outputs.shape:\",transformer_outputs.shape)\n",
    "        # Assuming transformer_outputs has the shape torch.Size([7, 50257])\n",
    "        transformer_outputs = transformer_outputs.unsqueeze(0)  # Add a new dimension at the beginning\n",
    "        #print(\"transformer_outputs shape:\", transformer_outputs.shape)  # torch.Size([1, 7, 50257])\n",
    "        keys, values =self.memory_bank(input_ids)\n",
    "        memory_augmented = self.memory_augmentation(keys, values, transformer_outputs)\n",
    "        #print(\"memory_augmented shape:\", memory_augmented.shape)\n",
    "        #input_tensor=transformer_outputs.view(-1, transformer_outputs.size(-1))\n",
    "        side_net_output = self.side_net(transformer_outputs)\n",
    "        #print(\"side_net_output shape:\", side_net_output.shape)\n",
    "        #side_net_output =side_net_output.view(-1, side_net_output.size(-1))\n",
    "        fused_output = self.memory_fusion(memory_augmented, side_net_output)\n",
    "        # Reshape transformer_output before passing to linear layer\n",
    "        batch_size, seq_length, hidden_dim = fused_output.size()\n",
    "        fused_output = fused_output.view(batch_size * seq_length,hidden_dim)\n",
    "        #print(\"forwardtransformer_outputnew.shape\",fused_output.shape)\n",
    "        logits = self.frozen_llm.lm_head(fused_output)\n",
    "        #logits = torch.matmul(fused_output, self.frozen_llm.linear.weight)\n",
    "        return logits,memory_augmented\n",
    "\n",
    "    def generate_text(self, input_ids, max_length):\n",
    "        with torch.no_grad():\n",
    "            output_ids = input_ids.clone()\n",
    "            for _ in range(max_length):\n",
    "                logits = model.forward(input_ids)\n",
    "                #print(\"logits.shape\",logits.shape)\n",
    "                logits=logits.unsqueeze(0)\n",
    "                predicted_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "                #print(\"predicted_token.shape\",predicted_token.shape)\n",
    "                predicted_token = predicted_token.unsqueeze(0)  # Unsqueeze along the batch dimension\n",
    "                #print(\"output_ids.shape\",output_ids.shape)\n",
    "                #print(\"predicted_token.shape\",predicted_token.shape)\n",
    "                output_ids= torch.cat((output_ids, predicted_token), dim=1)\n",
    "                input_ids = predicted_token  # Update input_ids for the next iteration\n",
    "        return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scalene\n",
      "  Obtaining dependency information for scalene from https://files.pythonhosted.org/packages/1d/c8/6e9387279894220720e6c9902a5614a3b9b88d7e56470d0454cb793cc6ea/scalene-1.5.26-cp39-cp39-manylinux_2_24_x86_64.whl.metadata\n",
      "  Downloading scalene-1.5.26-cp39-cp39-manylinux_2_24_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel>=0.36.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from scalene) (0.38.4)\n",
      "Requirement already satisfied: rich>=10.7.0 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from scalene) (13.4.1)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from scalene) (2.2.1)\n",
      "Collecting pynvml<11.5,>=11.0.0 (from scalene)\n",
      "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=3.0.3 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from scalene) (3.1.2)\n",
      "Requirement already satisfied: psutil>=5.9.2 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from scalene) (5.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from Jinja2>=3.0.3->scalene) (2.1.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from rich>=10.7.0->scalene) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from rich>=10.7.0->scalene) (2.15.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/anaconda3/envs/py39/lib/python3.9/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.7.0->scalene) (0.1.2)\n",
      "Downloading scalene-1.5.26-cp39-cp39-manylinux_2_24_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.5/773.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pynvml, scalene\n",
      "  Attempting uninstall: pynvml\n",
      "    Found existing installation: pynvml 11.5.0\n",
      "    Uninstalling pynvml-11.5.0:\n",
      "      Successfully uninstalled pynvml-11.5.0\n",
      "\u001b[33m  WARNING: The script scalene is installed in '/root/anaconda3/envs/py39/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pynvml-11.4.1 scalene-1.5.26\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U scalene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: scalene: not found\r\n"
     ]
    }
   ],
   "source": [
    "!scalene test/testme.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69118332eaf7486e976639624725e7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.82 GiB (GPU 0; 23.69 GiB total capacity; 20.24 GiB already allocated; 2.00 GiB free; 20.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#backbone_config = GPT2Config.from_pretrained(backbone_model_name)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLongMEMwithPretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbackbone_model_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Example input\u001b[39;00m\n\u001b[1;32m     18\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.82 GiB (GPU 0; 23.69 GiB total capacity; 20.24 GiB already allocated; 2.00 GiB free; 20.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/stable-vicuna-13B-HF\")\n",
    "# Example usage\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 32001\n",
    "hidden_dim = 32001\n",
    "num_layers = 12\n",
    "memory_dim = 32001\n",
    "backbone_model_name = \"TheBloke/stable-vicuna-13B-HF\"  # Change this if you have a different pretrained model\n",
    "#backbone_config = GPT2Config.from_pretrained(backbone_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LongMEMwithPretrained(vocab_size, embedding_dim, hidden_dim, memory_dim,backbone_model_name).to(device)\n",
    "\n",
    "# Example input\n",
    "input_ids = torch.tensor([[1, 2, 3, 4, 5]])\n",
    "\n",
    "# Forward pass\n",
    "output_logits = model(input_ids)\n",
    "\n",
    "print(\"Output logits shape:\", output_logits.shape)\n",
    "\n",
    "# Generate text\n",
    "max_length = 20\n",
    "generated_text = model.generate_text(input_ids, max_length)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configration and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "vocab_size = vocab_size\n",
    "embedding_dim = 768\n",
    "num_heads = 12\n",
    "hidden_dim = 768\n",
    "num_layers = 12\n",
    "memory_dim = vocab_size\n",
    "feedforward_size = 4 * hidden_dim\n",
    "dropout_rate = 0.1\n",
    "\n",
    "\n",
    "# Create an instance of LongMEM\n",
    "model = LongMEM(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers, memory_dim, feedforward_size, dropout_rate).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongMEM(\n",
       "  (frozen_llm): LargeLanguageModel(\n",
       "    (embedding): Embedding(51160, 768)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-11): 12 x TransformerBlock(\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feedforward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=768, out_features=51160, bias=True)\n",
       "  )\n",
       "  (memory_bank): CachedMemoryBank(\n",
       "    (embedding): Embedding(51160, 768)\n",
       "    (memory_key): Linear(in_features=768, out_features=51160, bias=True)\n",
       "    (memory_value): Linear(in_features=768, out_features=51160, bias=True)\n",
       "  )\n",
       "  (side_net): ResidualSideNet(\n",
       "    (fc1): Linear(in_features=51160, out_features=768, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=768, out_features=51160, bias=True)\n",
       "  )\n",
       "  (memory_augmentation): MemoryAugmentation()\n",
       "  (memory_fusion): MemoryRetrievalFusion(\n",
       "    (linear_query): Linear(in_features=51160, out_features=51160, bias=True)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing :Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      "[1212, 318, 257, 6291, 5128, 6827, 13]\n",
      "Input IDs:\n",
      "tensor([[1212,  318,  257, 6291, 5128, 6827,   13]])\n",
      "Input IDs shape:\n",
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "# Add a new pad token\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Encode a text sequence to obtain input_ids\n",
    "text = \"This is a sample input sentence.\"\n",
    "# Tokenize the input text and convert to input_ids\n",
    "tokens = tokenizer.encode(text)\n",
    "#input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(\"tokens:\")\n",
    "print(tokens)\n",
    "print(\"Input IDs:\")\n",
    "print(input_ids)\n",
    "print(\"Input IDs shape:\")\n",
    "print(input_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwardtransformer_output.shape torch.Size([1, 7, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([7, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1212,  318,  257, 6291, 5128, 6827,   13,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length=20\n",
    "Output_ids=model.generate_text(input_ids, max_length)\n",
    "Output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample input sentence.!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Convert the tensor to a list\n",
    "decoded_list = Output_ids.tolist()\n",
    "\n",
    "# Decode the list using the GPT-2 tokenizer\n",
    "decoded_text = tokenizer.decode(decoded_list[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the decoded text\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwardtransformer_output.shape torch.Size([1, 12, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([12, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "['### Human: What is Artificial Intelligence?  ### Assistant:!!!!!!!!!!']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text input\n",
    "text_input= '### Human: What is Artificial Intelligence?  ### Assistant:'\n",
    "\n",
    "# Tokenize the text input\n",
    "input_ids = tokenizer.encode(text_input, return_tensors=\"pt\")\n",
    "\n",
    "# Create LONGMEM model\n",
    "#longmem_model = LongMEM(vocab_size, embedding_dim, hidden_dim, num_layers, memory_dim, backbone_model_name, backbone_config)\n",
    "max_length=10\n",
    "outputs = model.generate_text(input_ids,max_length)\n",
    "print(tokenizer.batch_decode(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "        num_pages = pdf_reader.numPages\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.getPage(page_num)\n",
    "            text += page.extractText()\n",
    "\n",
    "    return text\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    pdf_file_path = \"demodoc.pdf\"\n",
    "#    extracted_text = pdf_to_text(pdf_file_path)\n",
    "#    print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = \"demodoc.pdf\"\n",
    "extracted_text = pdf_to_text(pdf_file_path)\n",
    "# Example text input\n",
    "text_input = extracted_text[1:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12792 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwardtransformer_output.shape torch.Size([1, 12792, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([12792, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "forwardtransformer_output.shape torch.Size([1, 1, 768])\n",
      "forwardtransformer_outputnew.shape torch.Size([1, 768])\n",
      "['rca: Progressive Learning from Complex\\nExplanation Traces of GPT-4\\nSubhabrata Mukherjee∗†, Arindam Mitra∗\\nGanesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah\\nMicrosoft Research\\nAbstract\\nRecent research has focused on enhancing the capability of smaller models\\nthrough imitation learning, drawing on the outputs generated by large\\nfoundation models (LFMs). A number of issues impact the quality of these\\nmodels, ranging from limited imitation signals from shallow LFM outputs;\\nsmall scale homogeneous training data; and most notably a lack of rigorous\\nevaluation resulting in overestimating the small model’s capability as they\\ntend to learn to imitate the style, but not the reasoning process of LFMs. To\\naddress these challenges, we develop Orca, a 13-billion parameter model\\nthat learns to imitate the reasoning process of LFMs. Orca learns from\\nrich signals from GPT-4 including explanation traces; step-by-step thought\\nprocesses; and other complex instructions, guided by teacher assistance from\\nChatGPT. To promote this progressive learning, we tap into large-scale and\\ndiverse imitation data with judicious sampling and selection. Orca surpasses\\nconventional state-of-the-art instruction-tuned models such as Vicuna-13B\\nby more than 100% in complex zero-shot reasoning benchmarks like Big-\\nBench Hard (BBH) and 42%on AGIEval. Moreover, Orca reaches parity\\nwith ChatGPT on the BBH benchmark and shows competitive performance\\n(4pts gap with optimized system message) in professional and academic\\nexaminations like the SAT, LSAT, GRE, and GMAT, both in zero-shot\\nsettings without CoT; while trailing behind GPT-4. Our research indicates\\nthat learning from step-by-step explanations, whether these are generated\\nby humans or more advanced AI models, is a promising direction to improve\\nmodel capabilities and skills.\\n∗Co-primary authors. Author contributions listed at the end of the paper.\\n†Correspondence to subhabrata.mukherjee@microsoft.com\\nWe are working with our legal team to publicly release a diff of the model weights in accordance\\nwith LLaMA’s release policy to be published at https://aka.ms/orca-lm.\\nWork in progress.arXiv:2306.02707v1  [cs.CL]  5 Jun 2023Contents\\n1 Introduction 4\\n1.1 Challenges with Existing Methods........................ 5\\n1.2 Key Contributions................................. 6\\n2 Preliminaries 7\\n2.1 Instruction Tuning................................. 7\\n2.2 Role of System Instructions............................ 7\\n3 Explanation Tuning 8\\n3.1 Dataset Construction................................ 8\\n3.1.1 System Messages.............................. 9\\n3.1.2 Dataset Description and Sampling from the FLAN-v2 Collection... 9\\n3.1.3 ChatGPT as Teaching Assistant..................... 12\\n3.2 Training....................................... 13\\n4 Experiment Setup 14\\n4.1 Baselines...................................... 14\\n4.2 Tasks......................................... 15\\n4.2.1 Open-ended Generation Capabilities................... 15\\n4.2.2 Reasoning Capabilities........................... 16\\n5 Evaluation for Open-ended Generation 17\\n6 Evaluation for Reasoning 17\\n6.1 AGIEval Results.................................. 17\\n6.2 Big-Bench Hard Results.............................. 20\\n7 Evaluation for Safety 23\\n7.1 Truthful Question Answering........................... 23\\n7.2 Toxic Content Generation............................. 26\\n7.3 Note on Hallucination and Tool Augmented LFMs............... 27\\n8 Limitations 28\\n9 Conclusions 29\\n10 Author Contributions 29\\n11 Case Studies 30\\n11.1 Trigonometric Problem Solving.......................... 30\\n11.2 Temporal Reasoning................................ 32\\n11.3 Multiple-choice Question-Answering....................... 33\\n211.4 Bio Olympiad.................................... 34\\n11.5 Forming Inequalities................................ 35\\n11.6 Counterfactual Question Answering........................ 38\\n11.7 Compound Interest Problems........................... 38\\n11.8 Question from Vicuna-Eval............................ 39\\n11.9 Spatial Reasoning.................................. 41\\n11.10Commonsense Question Answering........................ 42\\n11.11Hallucination.................................... 44\\n11.12Quadratic Equation Solving............................ 45\\n11.13Meeting Transcript Processing.......................... 46\\n31 Introduction\\n687692 93100103\\n20406080100120\\nLLaMA-13B Alpaca-13B Vicuna-13B Bard ChatGPT Orca-13BPerformance (%) relative to \\nChatGPTEvaluation with GPT -4 \\nFigure 1: Orca (13B params) outperforms a wide range of foundation models including Ope-\\nnAI ChatGPT as evaluated by GPT-4 in the Vicuna evaluation set. We further demonstrate\\nsimilar results against a wide range of evaluation sets from other works in experiments.\\n3042 4247\\n20253035404550\\nVicuna-13B Text-da-Vinci-003 Orca-13B ChatGPTAggregate  Accuracy  (%)Professional and Academic Exams (SAT, LSAT, GRE, GMAT)  (Zero -shot, MCQ)\\nFigure 2: Explanation tuning with Orca (13B params) bridges gap with OpenAI foundation\\nmodels like Text-da-Vinci-003 with 5 pts gap (the gap further reduces with optimized system\\nmessages) against ChatGPT across a wide range of professional and academic exams including\\nGRE, GMAT, LSAT, SAT from the AGIEval benchmark [ 1] in zero-shot settings (without\\nany exemplar or CoT). Topical performances shown in Figure 11.\\nLarge Foundation Models (LFMs) such as ChatGPT and GPT-4 [ 2] exhibit remarkable zero-\\nshot performances across a broad spectrum of tasks. Alongside academic benchmarks like\\nHuman Eval [ 3] and Big Bench [ 4], GPT-4 has also demonstrated human-level performance\\non various professional exams, including the bar exam, SAT, GRE, and USMLE. These\\nadvancements can be credited to the scaling of both model and dataset sizes, as well\\nas the incorporation of a second layer of training to better align the models with user\\nintent. This alignment is accomplished by fine-tuning the models via supervised learning on\\ndemonstrations of prompts and desired model behavior, and through reinforcement learning\\nfrom human preferences [5].\\nAs these models continue to evolve and become more powerful, an intriguing question arises:\\nCan we use the model itself to supervise its own behavior or that of other AI models? Bai\\net al. [6]have shown that by sampling output from an initial model, generating revisions,\\nand then fine-tuning the original model based on these revised responses, model behavior\\ncan be controlled more effectively and can be made more harmless, with significantly fewer\\nhuman labels.\\nRecently, there has been an influx of studies using LFMs like ChatGPT and GPT-4 as\\nteachers to generate large datasets, for instruction tuning, and to train smaller models,\\nsuch as Alpaca [ 7], WizardLM [ 8] and Vicuna [ 9]. While these models can produce content\\nthat matches the style of their teachers, they often fall short in terms of the reasoning and\\ncomprehension skills displayed by the larger foundation models.\\n423.348.9 49.7\\n0102030405060\\nVicuna-13B ChatGPT Orca-13BAggregate Accuracy (%)BigBench -Hard (Zero -shot, MCQ)Figure 3: For complex zero-shot reasoning tasks in BigBench-Hard, Orca achieves parity\\nwith ChatGPT (without any exemplar or CoT) with task performances shown in Figure 12.\\nTake, for example, the 13-billion parameter instruction-tuned model, Vicuna [ 9] (with\\nLLAMA-13B [ 10] as the base), which is widely regarded as one of the best models in its\\nfamily, as evidenced by its performance on leaderboards like OpenLLM3and ChatArena4.\\nAs illustrated in Figure 1, the widely-used evaluation method of using GPT-4 as the judge\\nsuggests that Vicuna retains 92%of ChatGPT’s quality. However, a more meticulous\\nevaluation on reasoning benchmarks against human labels finds Vicuna to retain only 64%\\nof ChatGPT’s quality on professional and academic exams (see Figure 2), and only 48%of\\nChatGPT’s quality on complex benchmarks like BigBench-hard [ 11] (see Figure 3)5. This\\ndiscrepancy not only underscores the limitations of existing evaluation protocols with smaller\\nLLMs, but it also reveals their significant lag in reasoning and comprehension capabilities.\\nIn essence, these models may be articulate, but they may not necessarily possess robust\\nreasoning skills. In this study, we discuss some of the reasons behind these gaps and propose\\nstrategies for addressing them.\\n1.1 Challenges with Existing Methods\\nCurrent research on instruction-tuning to mimic the output of LFM’s like ChatGPT exhibits\\nnotable limitation in task diversity, query complexity, and data scaling. These observations\\nare corroborated in a recent study by Gudibande et al. [12], where the authors assert that\\n“model imitation is a false promise” since “broadly matching ChatGPT using purely imitation\\nwould require (1) a concerted effort to collect enormous imitation datasets and (2) far more\\ndiverse and higher quality imitation data than is currently available.”. Contrary to this\\nassertion, we demonstrate that both conditions (1) and (2) are attainable and that it is\\npossible to reduce the gap with proprietary LLM’s on multiple zero-shot benchmarks that\\nrequire sophisticated reasoning. We elaborate on these challenges below:\\nSimple instructions with limited diversity. The Self-Instruct [ 13] process involves using\\nan initial set of prompts to incite the LFM to produce new instructions. Any low-quality or\\noverly similar responses are then removed, and the remaining instructions are reintegrated\\ninto the task pool for further iterations. Nonetheless, the resulting queries generated through\\nSelf-Instruct, such as “what are the three primary colors?\", “what is the capital of France?\", etc.,\\ncan exhibit limitations in diversity and complexity. Both Alpaca [ 7] and WizardLM [ 8]\\nemploy a variant of self-instruct. WizardLM introduces the concept of Evol-Instruct, which\\ngradually rewrites the initial set of instructions into more complex versions, attempting to\\novercome some of the method’s inherent shortcomings. On the other hand, recent works\\nlike Vicuna [ 9] and Koala [ 14] demonstrate remarkable performance due to more human-like\\nconversations and natural instructions in community-contributed conversations like those in\\nShareGPT6that provided a forum for users to share their conversations with ChatGPT.\\nTask diversity and data scaling. Human-contributed conversations in ShareGPT are a\\nvaluable source of data, but they also have some limitations. They tend to favor creative\\n3https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\\n4https://chat.lmsys.org/?arena\\n5ChatGPT may have data contamination issues with respect to BigBench\\n6https://sharegpt.com/\\n5content generation and information-seeking queries over other types of tasks. Therefore,\\nmodels trained on such natural conversations may capture the style but not the reasoning\\nprocess of the LFMs – demonstrated in the performance of Vicuna in Figures 2 and 3.\\nAdditionally, such mode of data collection is also limited in scale. Table 1 shows an overview\\nof the size of data and tuning methods employed in recent popular instruction tuning works.\\nLimited imitation signals. Existing methods rely on immitation learning from\\n⟨query, response ⟩pairs generated by the teacher model. However, this provides limited\\nsignals to trace the reasoning process of the teacher. Prior works [ 15,16] on open-box model\\nshow that richer signals such as logits, intermediate representations and attention states can\\nsignificantly improve distillation performance. While they are not accessible for closed-box\\nLFM’s7, recent work [ 17] demonstrates that richer signals like LFM rationales can help close\\nthe gap for task-specific distillation.\\nEvaluation: Previous studies on instruction tuning of small models with LFMs are severely\\nlimited in their evaluation protocol. They often rely on GPT-4 for auto-evaluation by asking\\nit to compare the outputs of two systems with a prompt like “given responses from system\\n1 (reference) and system 2 (target), which one is better?”. However, this approach has\\nseveral drawbacks, such as the small size of test sets (e.g., 80instructions in Vicuna and 218\\ninstructions in WizardLM) and the biases of GPT-4 as the judge [ 18]. For example, we notice\\nthat models that are instruction-tuned with GPT-4 responses tend to generate longer texts\\nthat GPT-4 prefers over shorter ones; as well as GPT-4 has a bias in the order of the candidate\\nresponses. We will show that such auto-evaluation measures overestimate the abilities of\\nsmaller models compared to LFMs, as the former are much weaker in comprehension and\\nreasoning skills.\\n1.2 Key Contributions\\nIn this research, our focus is on addressing the challenges mentioned above, specifically with:\\nExplanation tuning: We augment ⟨query, response ⟩pairs with detailed responses from\\nGPT-4 that explain the reasoning process of the teacher as it generates the response. These\\nprovide the student with additional signals for learning. We leverage system instructions (e.g..,\\nexplain like I’m five, think step-by-step and justify your response, etc.) to\\nelicit such explanations. This is in contrast to vanilla instruction tuning, which only uses the\\nprompt and the LFM response for learning, providing little opportunity for mimicking the\\nLFM’s “thought” process.\\nScaling tasks and instructions: We utilize the Flan 2022 Collection [ 19] as it provides\\nan extensive public assortment of tasks and instructions. Particularly, we use FLAN-\\nv2, supplemented with high-quality templates, advanced formatting patterns, and data\\naugmentations. Even though FLAN holds tens of millions of instructions, we selectively\\nsample from the task collection to form a diverse mixture of tasks, which we then further\\nsub-sample to generate complex prompts. These prompts are used to query LFMs like\\nChatGPT and GPT-4, thus creating a rich and diverse training set. We collect 5million\\nChatGPT responses, from which 1million is further sampled to acquire GPT-4 responses.\\nWe demonstrate how ChatGPT as a teacher assistant helps in progressive learning.\\nEvaluation: We assess the generative, reasoning, and comprehension abilities of Orca, under\\na range of settings: (i) AutoEvaluation with GPT-4 on existing evaluation sets from Vicuna,\\nWizardLM and the awesome prompts collection8; (ii) Academic benchmarks like Big-Bench\\nHard [ 4] and TruthfulQA [ 20]; (iii) Professional and Academic exams like SAT, LSAT, GRE,\\nGMAT from AGIEval [ 1]; (iv) Safety evaluation with ToxiGen [ 21] to test toxic language\\ngeneration and hate speech detection across different minority groups. Finally, we provide\\ncase-studies to compare the generation and reasoning abilities of Orca against OpenAI LFMs\\nlike ChatGPT and GPT-4, and instruction-tuned smaller model like Vicuna.\\n7Note that OpenAI API’s do give access to the top-5logits for each token.\\n8https://prompts.chat/\\n6Model Tuning Method Data Size Teacher\\nAlpaca Simple Instructions / Self-instruct 52K text-da-vinci-003\\nVicuna User Instructions / Natural 70K ChatGPT\\nDolly User Instructions / Natural 15K Human\\nWizardLM Complex Instructions / Evol-instruct 250K ChatGPT\\nOrca Complex Instructions / Explanations 5M ChatGPT (5M)\\n∩GPT-4 (1M)\\nTable 1: Overview of popular models instruction tuned with OpenAI large foundation models\\n(LFMs). Orca leverages complex instructions and explanations for progressive learning.\\nUser Instruction: Use the given data to calculate the median. \\nInput: [7, 3, 8, 2, 10]\\nUser Instruction: Answer this question.\\nInput: Which small lake lies between Windermere and Grasmere?User Instruction: In this task, you will be presented with a question having \\nmultiple possible answers in Italian language. And you should choose a most \\nsuitable option out of \"A\", \"B\", \"C\", \"D\", and \"E\" based on your commonsense \\nknowledge. \\nInput: Solve this question: Dove non riusciresti a vedere la luce? \\nOptions: A scrivaniaB frigoriferoC sole D universoE atticoOutput: First, we need to arrange the data in ascending order: [2, 3, 7, 8, 10]. \\nSince there are 5 numbers, the median is the middle number, which is 7.\\nOutput: B frigorifero\\nOutput: Rydal Water lies between Windermere and Grasmere.\\nFigure 4: Instruction-tuning with GPT-49. Given user instructions for a task and an input,\\nthe system generates a response. Existing works like Alpaca [ 7], Vicuna [ 9] and variants\\nfollow a similar template to train small models with ⟨{user instruction, input}, output ⟩.\\n2 Preliminaries\\n2.1 Instruction Tuning\\nInstruction tuning [ 22] is a technique that allows pre-trained language models to learn\\nfrom input (natural language descriptions of the task) and response pairs, for example,\\n{\"instruction\": \"Arrange the words in the given sentence to form a grammatically\\ncorrect sentence.\", \"input\": \"the quickly brown fox jumped\", \"output\": \"the brown\\nfox jumped quickly\"}. Instruction tuning has been applied to both language-only and\\nmultimodal tasks. For language-only tasks, instruction tuning has been shown to improve\\nthe zero-shot and few-shot performance of models such as FLAN [ 22] and InstructGPT [ 5]\\non various benchmarks. For multimodal tasks, instruction tuning has been used to generate\\nsynthetic instruction-following data for language-image tasks, such as image captioning [ 23]\\nand visual question answering [24].\\nA wide range of works in recent times, including Alpaca [ 7], Vicuna [ 9], WizardLM [ 8] and\\nKoala [ 14], have adopted instruction-tuning to train smaller language models with outputs\\ngenerated from large foundation models from the GPT family. As outlined in Section 1.1,\\na significant drawback with all these works has been both limited task diversity, query\\ncomplexity and small-scale training data in addition to limited evaluation overstating the\\nbenefits of such approach.\\n2.2 Role of System Instructions\\nVanilla instruction-tuning (refer to Figure 4 for examples) often uses input, response pairs\\nwith short and terse responses. Such responses when used to train smaller models, as in\\nexisting works, give them limited ability to trace the reasoning process of the LFM. In\\nconstrast, system instructions10in recent LFMs like GPT-4 can be used to provide guidance\\n9GPT-4 inference hyper-parameters in Azure OpenAI interface set as: temperature=0.7,\\ntop_p=0.95, frequency_penalty=0, presence_penalty=0, stop=None.\\n10System instructions are part of the Chat Completion API, which is a new dedicated API for\\ninteracting with the ChatGPT and GPT-4 models.\\n7System Instruction: You are an AI assistant. User will you give you a task. Your \\ngoal is to complete the task as faithfully as you can. While performing the task \\nthink step-by-step and justify your steps.\\nUser Instruction: Use the given data to calculate the median.\\nInput: [7, 3, 8, 2, 10]\\nSystem Instruction: You are an AI assistant. User will you give you a task. Your \\ngoal is to complete the task as faithfully as you can. While performing the task \\nthink step-by-step and justify your steps.\\nUser Instruction: Answer this question.\\nInput: Which small lake lies between Windermere and Grasmere?System Instruction: You are an AI assistant. Provide a detailed answer so user \\ndon\\'t need to search outside to understand the answer.\\nUser Instruction: In this task, you will be presented with a question having \\nmultiple possible answers in Italian language. And you should choose a most \\nsuitable option out of \"A\", \"B\", \"C\", \"D\", and \"E\" based on your commonsense \\nknowledge. \\nInput: Solve this question: Dove non riuscirestia vedere la luce? \\nOptions: A scrivaniaB frigoriferoC sole D universoE atticoOutput: To calculate the median, I will follow these steps:\\nStep 1: Arrange the data in ascending order [2, 3, 7, 8, 10]\\nStep 2: Determine if the data set has an odd or even number of values\\nThe data set has 5 values, which is an odd number.\\nStep 3: Find the middle value\\nSince the data set has an odd number of values, the median is the middle \\nvalue in the ordered list. The middle value in the list [2, 3, 7, 8, 10] is 7.\\nOutput: Based on the given options and the context of the question, \"Dove \\nnon riusciresti a vedere la luce?\" (Where would you not be able to see \\nlight?), the most suitable answer would be:B frigorifero(refrigerator). \\nThis is because when the refrigerator door is closed, you would not be able \\nto see the light inside it. In the other options, light is generally visible or \\npresent:\\nA scrivania(desk) -Light is usually present around a desk. \\nC sole (sun) -The sun is the primary source of light in our solar system.\\nD universo(universe) -Light is present throughout the universe in the form \\nof stars, galaxies, etc.\\nE attico(attic) -Attics typically have windows or other sources of light, \\nmaking it possible to see light there.\\nOutput: To answer this question, I will search for information about the small \\nlake located between Windermere and Grasmere.\\nStep 1: Identify the location of Windermere and Grasmere. They are both \\nlocated in the Lake District National Park, in the county of Cumbria, England.\\nStep 2: Research the lakes in the Lake District. The Lake District is known for \\nits many lakes, so I will look for a list of lakes in the area and their locations in \\nrelation to Windermere and Grasmere.\\nStep 3: Find the small lake between Windermere and Grasmere. \\nAfter researching the lakes in the Lake District, I found that Rydal Water is a \\nsmall lake situated between Windermere and Grasmere.Figure 5: Explanation-tuning with GPT-4. In addition to user instructions and input, system\\ninstructions are provided to guide the system to form a well-reasoned and cogent response.\\nSystem instructions are sampled from a diverse instruction set including chain-of-thought\\nreasoning steps, explain like I’m five, being helpful and informative, etc. Such rich and\\nwell-structured response allows tuning small models to mimic the thinking process of GPT-4\\non⟨{system instruction, user instruction, input}, output ⟩pairs.\\nto the model on how to behave and respond. They are written in natural language and\\nseparated from the user messages by using the role of “system” in the JSON request. System\\ninstructions can specify the tone, task, format, and limitations of the model’s responses.\\nSystem instructions are also a way of improving the safety of model responses. For example,\\na set of system instructions designed for safety harness could be:\\n•The assistant must not generate harmful or offensive content.\\n•The assistant must respect the privacy and consent of the user.\\n•The assistant must acknowledge its limitations and uncertainties.\\n3 Explanation Tuning\\nTo address the shortcomings of existing works, we tap into large-scale training data with\\ndiverse tasks augmented with complex instructions and rich signals. Specifically, our data\\ncontains human and augmented system instructions for a large collection of tasks sampled\\nfrom FLAN-v2 (aka Flan 2022) [ 19]. Given the large size of the FLAN-v2 collection and\\nvarying number of examples for constituent datasets and tasks, we sample from a mixture of\\ntasks from different categories (described in the next section) to create our training data.\\n3.1 Dataset Construction\\nEach instance in our training data consists of the following triple: ⟨System message, User\\nquery, LFM response ⟩. Thesystem message, placed at the start of the prompt, provides\\nthe LFM with essential context, guidelines, and other pertinent details. We leverage the\\nsystem message to vary the length of the response; outline the assistant’s character; establish\\nacceptable and non-acceptable LFM behavior; and determine the structure of the agent’s\\nresponse. The user query defines the actual task we want the LFM to perform. To obtain\\na large and diverse set of user queries we utilize the FLAN-v2 collection [ 19]. We sample 5\\nmillion user queries from FLAN-v2 for which we collect ChatGPT responses. We further\\nsample 1million instructions from the 5million set for which we collect GPT-4 responses.\\nAll the queries to the agents are augmented with system instructions, as outlined below.\\n83.1.1 System Messages\\nWe hand-craft a total of 16system messages designed to evoke different kinds of responses\\nfrom the LFM. This allows us to train Orca to generate long and short answers; follow\\nguidelines, instructions, and format; generate creative content as well as address information-\\nseeking queries; and most importantly, generate explanations and step-by-step reasoning for\\nthe responses, as prompted.\\nId. System Message\\n1 <empty system message>\\n2You are an AI assistant. Provide a detailed answer so user don’t need to search outside to\\nunderstand the answer.\\n3You are an AI assistant. You will be given a task. You must generate a detailed and long\\nanswer.\\n4You are a helpful assistant, who always provide explanation. Think like you are answering\\nto a five year old.\\n5You are an AI assistant that follows instruction extremely well. Help as much as you can.\\n6You are an AI assistant that helps people find information. Provide a detailed answer so\\nuser don’t need to search outside to understand the answer.\\n7You are an AI assistant. User will you give you a task. Your goal is to complete the task\\nas faithfully as you can. While performing the task think step-by-step and justify your\\nsteps.\\n8You should describe the task and explain your answer. While answering a multiple choice\\nquestion, first output the correct answer(s). Then explain why other answers are wrong.\\nThink like you are answering to a five year old.\\n9 Explain how you used the definition to come up with the answer.\\n10You are an AI assistant. You should describe the task and explain your answer. While\\nanswering a multiple choice question, first output the correct answer(s). Then explain\\nwhy other answers are wrong. You might need to use additional knowledge to answer the\\nquestion.\\n11You are an AI assistant that helps people find information. User will you give you a\\nquestion. Your task is to answer as faithfully as you can. While answering think step-by-\\nstep and justify your answer.\\n12 User will you give you a task with some instruction. Your job is follow the instructions as\\nfaithfully as you can. While answering think step-by-step and justify your answer.\\n13 You are a teacher. Given a task, you explain in simple steps what the task is asking, any\\nguidelines it provides and how to use those guidelines to find the answer.\\n14 You are an AI assistant, who knows every language and how to translate one language to\\nanother. Given a task, you explain in simple steps what the task is asking, any guidelines\\nthat it provides. You solve the task and show how you used the guidelines to solve the\\ntask.\\n15Given a definition of a task and a sample input, break the definition into small parts.\\nEach of those parts will have some instruction. Explain their meaning by showing an\\nexample that meets the criteria in the instruction. Use the following format:\\nPart #: a key part of the definition.\\nUsage: Sample response that meets the criteria from the key part. Explain why you think\\nit meets the criteria.\\n16 You are an AI assistant that helps people find information.\\nTable 2: System instructions used to augment user instructions and task descriptions to\\nquery large foundation models for explanation tuning. System messages are designed to\\npreserve the ability of the model to generate both short and long answers.\\nWe have crafted different system messages for different sub-collections of the FLAN-v2\\ncollection. Table 2 lists all the system instructions used to generate our training data.\\nFigure 6 shows the distribution of system messages across different sub-collections. Note that\\nsystem message #8 and system message#10 are sampled only for multiple-choice questions;\\nthus they are less in number.\\n3.1.2 Dataset Description and Sampling from the FLAN-v2 Collection\\nThe FLAN-v2 Collection [ 19] consists of five sub-collections, namely, CoT, NiV2, T0, Flan\\n2021, Dialogue. Each sub-collection contains multiple tasks, where each task is a collection\\n91 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160.000.050.100.150.200.250.300.35frequencysub-collection = COT\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16sub-collection = NiV2\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\\nsystem message id0.000.050.100.150.200.250.300.35frequencysub-collection = T0\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\\nsystem message idsub-collection = Flan2021Figure 6: Relative frequency of system messages in different collections of our training data.\\nof queries. Each sub-collection is associated with multiple academic datasets. One or more\\ntasks are created from each dataset, focusing on zero shot and few-shot queries. In this\\nwork, we sample only zero-shot queries for training Orca. We have not sampled from the Di-\\nalogue sub-collection as the queries often lack context to elicit useful response from ChatGPT.\\nMixture Name Sampling Algorithm Original Size Sampled\\nCoT No Sampling 150K 150K\\nNIV2 Stratified Sampling 5M 440K\\nFLAN2021 Stratified Sampling >28.9M 2.5M\\nT0 Stratified Sampling 85.7M 2M\\nDialog Skipped 22.5M 0\\nTable 3: Construction of our training data with 5million samples.\\nZero-Shot CoT : The zero-shot Chain-Of-Thought (CoT) collection contains a total of\\n18 tasks involving math word problem solving, natural language inference, common-sense\\nreasoning, science question answering, odd-one-out reasoning. Each query in this dataset\\nis geared for chain-of-thought response. Figure 7 shows a demonstrative example from the\\nCoT collection. It also shows the human written response that is part of the original dataset,\\nChatGPT generated response and GPT-4 response. While all the sources agree on the final\\nanswer, ChatGPT and GPT-4 augmented with system instructions generate quite detailed\\nresponses as opposed to the human authored answer. The 18tasks contain approximately\\n150thousand queries in total. We include all of them in our 5million set.\\nNiV2 : NiV2 contains a total of 1560tasks and roughly 5million queries. The number of\\nqueries vary in each task. Figure 8 shows the distribution of the tasks along with task types.\\nWe randomly sample 300queries from each task, and the entire task data containing lesser\\nnumber of queries. This gives us a total of 440thousand queries.\\nFlan 2021 : Flan 2021 collection contains a total of 142tasks that are created from 62\\ndatasets. The entire Flan 2021 collection from the published script is quite large and\\nexpensive to augment. In order to sample a diverse and representative subset, we generate\\nat most 1million queries from each task, which gives us roughly 28.9million queries in total.\\nEach task in this collection contains varying number of queries. We sample a total of 2.5\\nmillion queries from this collection of 85.7 million queries. The sampling process is outlined\\nin Algorithm 1.\\n10### System:\\nYou are an AI assistant that helps people find information. User will you give you a question.\\nYour task is to answer as faithfully as you can. While answering think step-by-step and\\njustify your answer.\\n### Human:\\nPick which sentence is not logical.\\nOptions:\\n- Sentence A: \"people in the desert often look forward to flood\"\\n- Sentence B: \"people in the desert often look forward to rain\"\\nHmmm, let me think. I want to lay out the solution in details.\\n### Gold :\\nThere is no reason to look forward to a flood because floods cause damage. The answer is\\nSentence A.\\n### ChatGPT :\\nSure, let’s break it down step by step.\\n1. Read the sentences carefully.\\n2. Identify the difference between the two sentences.\\n3. Determine which sentence is not logical based on the difference.\\nNow let’s apply these steps to the given sentences:\\n1. Sentence A: \"people in the desert often look forward to flood\"\\n- This sentence suggests that people in the desert are eagerly anticipating a flood. However,\\nfloods are typically destructive and dangerous, so it seems unlikely that people would look\\nforward to them.\\n2. Sentence B: \"people in the desert often look forward to rain\"\\n- This sentence suggests that people in the desert are eagerly anticipating rain. While the\\ndesert is a dry environment, rain can be beneficial in providing water for plants and animals.\\n3. Based on the above analysis, it can be concluded that Sentence A is not logical as it\\nsuggests that people in the desert look forward to something that is typically harmful and\\ndangerous. Therefore, the answer is Sentence A: \"people in the desert often look forward to\\nflood\".\\n### GPT4 :\\nTo determine which sentence is not logical, let’s examine each sentence and its meaning in\\nthe context of people living in a desert.\\nSentence A: \"people in the desert often look forward to flood\"\\n- This sentence implies that people living in a desert are eagerly awaiting a flood. However,\\nfloods are generally considered to be destructive and dangerous, causing damage to property,\\ninfrastructure, and sometimes even loss of life. It is not logical for people to look forward to\\nsuch a disastrous event.\\nSentence B: \"people in the desert often look forward to rain\"\\n- This sentence implies that people living in a desert are eagerly awaiting rain. Rain is\\ntypically beneficial in a desert environment, as it provides water for drinking, agriculture,\\nand replenishing natural resources. It is logical for people living in a desert to look forward\\nto rain, as it can bring relief from the harsh, dry conditions.\\nBased on the analysis of both sentences, it can be concluded that Sentence A (“people\\nin the desert often look forward to flood”) is not logical, as it suggests that people are\\neagerly anticipating a destructive event, which is not a reasonable expectation. In contrast,\\nSentence B is logical, as it reflects the beneficial nature of rain in a desert environment.\\nFigure 7: Demonstrative example from Flan-CoT Collection.\\n111 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12 2 2 2 2 2 23 3 3 3 3 34 4 4 4 456 6 6 67 71012 12 12 13 14 15 16 17 192430 32 32 35 37 39 39 42 44 51 53 59 65 69 76 78 8095 101122 137 143207 211278# of Tasks\\n1101001000\\nImage CaptionArtAstronomyBusiness EthicsEconometricsElectrical EngineeringHuman SexualityInternational LawFormal FallacyManagementMarketingMedical GeneticsPrehistoryAccountingPublic RelationsSecurity: National SecurityUS Foreign PolicyPop CultureFormal logicSQLLiteraturePsychologyMoral ScenariosPhilosophyConferenceChemistryGeographyEconomicsJurisprudenceAnimalsGameDebatepediaLogicPhysicsNutritionProfessionsPersonal NarrativesLinguisticsEnglish ExamsGlobal FactsWorld ReligionsStatisticsSportsMedicineStereotypesFoodCountriesMoviesScientific Research PapersComputer ScienceHealthcareBiologyKnowledge BaseAnthropologyJusticeBooksSchool Science TextbooksGovernment and PoliticsCodePublic PlacesFictionWebStoryLawNatural ScienceNarrativeReviewsSocial MediaMiscellaneousHistoryMathematicsTED TalksDialogueSociologyCaptionsCommonsenseWikipediaNewsFigure 8: NiV2 task distribution reproduced from [25].\\nAlgorithm 1: Sampling Algorithm for Flan 2021 and T0 collection.\\nInput: tasks T={t1, t2,..., t m}, number of queries to sample n\\nOutput: sampled queries Q={q1, q2,..., q n}\\nQ←empty list\\nwhile|Q|< ndo\\nt←randomly sample a task from T\\nq←randomly sample a query without replacement from t\\nadd qtoQ\\niftis empty then\\nremove tfrom T\\nend\\nend\\nreturn Q\\nT0: T0 collection contains a total of 193tasks that are associated with 35training datasets.\\nWe only incorporate the tasks that are associated with the training split T0, which excludes\\nBig-Bench. This is important as we include Big-Bench-Hard in our evaluation benchmark.\\nT0 collection contains roughly 85.7million queries with the number of queries varying in\\neach task. We sample a total of 2million queries from this collection using the sampling\\nprocess in Algorithm 1.\\n3.1.3 ChatGPT as Teaching Assistant\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1602004006008001000120014001600gpt4\\nchatgpt\\nFigure 9: Comparing GPT-4 and ChatGPT response length distribution for different system\\nmessages. The system message ids {1,2,..., 16}correspond to the row numbers in Table 2.\\nWe observe GPT-4 to elicit longer responses compared to ChatGPT.\\n12Teacher Cost per 1000token Requests per\\nminuteTokens per\\nminute\\nChatGPT $0.002 300 120,000\\nGPT-4 (8K) $0.03 (prompt),\\n$0.06(token com-\\npletion)18 10,000\\nTable 4: Pricing and quota limit for data collection from ChatGPT (GPT-3.5-turbo) and\\nGPT-4 endpoints using Azure OpenAI service.\\nWe generate 5million instructions (queries augmented with system messages) referred\\nas FLAN-5M following sampling techniques outlined in the previous section. We further\\nrandomly sample 1million queries from FLAN-5M to create another split, referred as\\nFLAN-1M. We use Azure OpenAI API11to collect ChatGPT (GPT-3.5-turbo) responses to\\nFLAN-5M, and GPT-4 responses to FLAN-1M.\\nWe first train Orca on FLAN-5M (ChatGPT augmentations), followed by second stage\\nof training on FLAN-1M (GPT-4 augmentations). Essentially, we leverage ChatGPT as\\nintermediate teacher assistant for two reasons.\\n•Capacity gap: Orca with 13Bparameters is many times smaller than GPT-4 (size\\nundisclosed). Leveraging an intermediate teacher with reduced gap in capabilities, in this\\ncase ChatGPT, has been shown to improve imitation learning performance for smaller\\nstudents in knowledge distillation [ 15]. This can be viewed as a form of progressive\\nlearning or curriculum learning, where the student first learns from easier examples,\\nfollowed by harder ones: with the assumption that longer responses are difficult to mimic\\nthan shorter ones, along with improved reasoning and step-by-step explanation from a\\nlarger teacher.\\n•Cost12andTime13: Large-scaledatacollectionfromAzureOpenAIAPI’sareconstrained\\nby, (a) rate limit in terms of allowed requests per minute to prevent throttling the\\nendpoints, (b) available tokens per minute due to serving latency, and (c) the dollar cost\\nfor length of prompt and token completion (demonstrated in Table 4) with the ChatGPT\\nAPI being much faster and cheaper than the GPT-4 endpoint. To this end, we collect\\n5×as much data from ChatGPT compared to GPT-4.\\nFigure 9 shows the response length distribution for ChatGPT and GPT-4 corresponing to\\ndifferent system messages. We observe that GPT-4 responses are on an average 1.5×longer\\nthan that of ChatGPT. This allows Orca to progressively learn from increasing complexity\\nof teacher explanations. We demonstrate the impact of teacher assistance via ablation\\nexperiments.\\n3.2 Training\\nThis section provides an overview of the training process for Orca, covering different aspects\\nof tokenization, sequencing, and loss computation.\\nTokenization : We utilize the LLaMA Byte Pair Encoding (BPE) tokenizer for processing\\nthe input examples. Notably, the LLaMA tokenizer splits all numbers into individual digits,\\nand fallbacks to bytes to decompose unknown UTF-8 characters. To deal with variable\\nlength sequences we add a padding token “[[PAD]]” into the LLaMA tokenizer vocabulary.\\nThe resulting vocabulary contains 32,001tokens.\\nPacking : To optimize the training process and utilize the available computational resources\\nefficiently, we employ the packing technique [ 26]. This method involves concatenating\\n11https://azure.microsoft.com/en-us/products/cognitive-services/openai-service/\\n12https://azure.microsoft.com/en-us/pricing/details/cognitive-services/\\nopenai-service/\\n13https://learn.microsoft.com/en-us/azure/cognitive-services/openai/quotas-limits\\n13multiple input examples into a single sequence, which is then used for training the model.\\nThe packing is performed such that the total length of the concatenated sequence does\\nnot exceed max_len = 2,048tokens. Particularly, we shuffle the input examples and then\\npartition the examples into groups such that length of the concatenated sequence in each\\ngroup is at most max_len. Padding tokens are then added to the concatenated sequence to\\nachieve a uniform input sequence length of max_len with a packing factor of 2.7examples\\nper sequence given the length distribution of augmented instructions in our training data.\\nLoss: For the purpose of training Orca, we compute the loss onlyon the tokens generated\\nby the teacher model, i.e., it learns to generate responses conditioned on the system message\\nand task instructions. This approach ensures that the model focuses on learning from the\\nmost relevant and informative tokens, improving the overall efficiency and effectiveness of\\nthe training process.\\nCompute: We trained Orca on 20NVIDIA A 100GPUs with 80GB memory. It took 160\\nhours to train Orca on FLAN-5M (ChatGPT augmentations) for 4epochs, and 40hours to\\ncontinue training on FLAN-1M (GPT-4 augmentations) for the same number of epochs.\\nIt took 2 weeks and 3 weeks respectively to collect data from GPT-3.5-turbo (ChatGPT)\\nand GPT-4 from multiple endpoints accounting for the throttling limit, endpoint load, and\\nlength distribution of query and response pairs.\\n4 Experiment Setup\\nWe setup a rigorous evaluation protocol that considers a host of different abilities including\\nwriting, comprehension, analytical, mathematical and logical reasoning.\\n4.1 Baselines\\nWe compare Orca14against the following baselines:\\n•Text-Davinci-003 (TD-003): Text-Davinci-003 belong to the GPT-3.515series of\\ngeneration model that is optimized for text completion. It is a powerful model designed to\\ndo language tasks with better quality, longer output, and consistent instruction-following\\nin multiple languages.\\n•ChatGPT: ChatGPT (GPT-3.5-turbo) is the most capable GPT-3.5 model and an\\nimprovement on text-davinci-003. It is optimized for chat and trained using conversations\\nwith humans. OpenAI released this chatbot in November 2022.\\n•GPT-4: GPT-4 is the latest model in the GPT family and exhibits human-level per-\\nformance on various professional and academic benchmarks. Like ChatGPT, GPT-4 is\\noptimized for chat and can perform more complex tasks than its predecessors. It typically\\nshows substantially higher performance than GPT-3.5 models, especially on tasks that\\nrequire complex reasoning. For both ChatGPT and GPT-4, we use the OpenAI API\\nversion “2023-03-15-preview\".\\n•Vicuna: Vicuna [ 9] is an open-source chatbot that was trained by fine-tuning LLaMA[ 10]\\non user-shared conversations collected from ShareGPT. In this work, we use the Vicuna\\nmodel consisting of 13B parameters. Vicuna has been the leading open-source language\\nmodel in multiple leaderboards including Chatbot Arena16and Open LLM Leaderboard17.\\nWe used Vicuna model checkpoint current as of April 21, 2023.\\n14Unless specified otherwise, we use <empty system message> (Id. 1 from Table 2) as the system\\nmessage, temperature as 0.7 for Orca in all our experiments.\\n15https://platform.openai.com/docs/models/gpt-3-5\\n16https://lmsys.org/blog/2023-05-03-arena/\\n17https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\\n14Dataset Task Type # Examples\\nVicuna Prompts [9] Open-ended questions and generation 80\\nAwesome Prompts [27] Open-ended questions and generation 164\\nWizardLM Prompts [8] Open-ended questions and generation 218\\nAGIEval [1] Suite of professional and academic exams /\\nmultiple-choice questions3546\\nBig-Bench Hard [11] Suite of complex reasoning tasks / multiple-\\nchoice questions5511\\nTable 5: Orca evaluation benchmarks. Dataset statistics.\\n### System: You are a helpful and precise assistant for checking the quality of the\\nanswer.\\n### Human:\\n[Question]\\nQuestion\\n[The Start of Assistant 1’s Answer]\\nAnswer 1\\n[The Start of Assistant 2’s Answer]\\nAnswer 2\\n[System]\\nWe would like to request your feedback on the performance of two AI assistants in\\nresponse to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses.\\nEach assistant receives an overall score on a scale of 1 to 10, where a higher score\\nindicates better overall performance.\\nPlease first output a single line containing only two values indicating the scores\\nfor Assistant 1 and 2, respectively. The two scores are separated by a space. In\\nthe subsequent line, please provide a comprehensive explanation of your evaluation,\\navoiding any potential bias and ensuring that the order in which the responses were\\npresented does not affect your judgment.\\n### Assistant:\\nFigure 10: Prompt template from Vicuna [ 9] to rate the writing quality of the candidate\\nassistant model against the reference model (e.g., ChatGPT, GPT-4).\\n4.2 Tasks\\nWe provide a detailed account of the tasks used to evaluate Orca’s capability in terms of\\nopen-ended generation and its ability to reason and comprehend complex reasoning tasks in\\nthis section. Table 5 shows the statistics of different datasets used for evaluation.\\n4.2.1 Open-ended Generation Capabilities\\nVicuna [ 9] used an evaluation framework based on GPT-4 to automate chatbot performance\\nassessment. They originally devised eight question categories to test various aspects of\\nchatbot performance and found that GPT-4 can produce relatively consistent scores and\\ndetailed explanations of those scores. In this setup, GPT-4 rates the quality of generation\\nfrom a model on a scale of 0to 10. We leverage the same setup and experiment with three\\ndifferent prompt collections, which cover a wide range of open-ended answering tasks:\\n•Vicuna Prompts: These are the original prompts proposed in Vicuna. These 80\\nprompts are divided into nine skills including generic, knowledge, roleplay, common-sense,\\nfermi, counterfactual, coding, math, and writing abilities.\\n•Awesome Prompts: Awesome ChatGPT prompts [ 27] is a collection of prompt examples\\nthat are primarily used with the ChatGPT model as reference.\\n15Exams #Participants Tasks Subject # Exam-\\nples# Avg.\\nToken\\nGRE 340KAQuA-RAT Math 254 77GMAT 150K\\nCivil Service\\nExamination2M LogiQA Logic 651 144\\nLaw School\\n170KLSAT-AR Law-Analytics 230 154\\nAdmission LSAT-LR Law-Logic 510 178\\nTest (LSAT) LSAT-RC Law-Reading 260 581\\nSAT 1.7MSAT-English English 206 656\\nSAT-Math Math 220 54\\nTable 6: Breakdown of tasks in AGIEval benchmark as reported in [ 1]. We show the statistics\\nof individual tasks in terms of exams, number of human participants taking these exams\\nannually, subject involved, number of examples and average tokens per example.\\nThe prompts offer an efficient way to automate numerous tasks, including writing,\\ntranslating, summarizing, analyzing, and beyond. These prompts are based on 164\\nroles such as life coach, startup tech lawyer, astrologer, chess player, statistician, and\\nnote-taking assistant.\\n•WizardLM Prompts: WizardLM prompts [ 8] are a collection of prompt examples based\\non real-world tasks. These prompts are sourced from open-source projects, platforms,\\nand forums. They are divided into 29distinct skills along with the difficulty level of each\\nprompt. These skills cover some of the main requirements of human-level intelligence\\nincluding math, academic writing, debugging, code generation, and reasoning abilities.\\n4.2.2 Reasoning Capabilities\\n•AGIEval: AGIEval [ 1] is a human-centric benchmark that evaluates the general abilities\\nof foundation models in tasks related to human cognition and problem-solving. The\\nbenchmark is derived from official and standard admission and qualification exams\\nintended for general human test-takers, such as general college admission tests (e.g.,\\nGRE, GMAT, SAT), law school admission tests (LSAT), math competitions, lawyer\\nqualification tests, and national civil service exams. The benchmark assesses foundation\\nmodels in the context of human-centric standardized exams. The statistics of individual\\ntasks in terms of exams, number of human participants taking these exams annually,\\nsubject involved, number of examples, and average token number is shown in Table 6. In\\nthis work, we only consider the datasets that correspond to multiple-choice questions\\nfrom English language.\\n•Big-Bench Hard (BBH): BIG-Bench Hard is a suite of 23challenging BIG-Bench [ 4]\\ntasks that were introduced to measure the capabilities and limitations of large language\\nmodels. Thesearethetasksforwhichpriorlanguagemodelevaluationsdidnotoutperform\\nthe average human-rater. In this work, we only use the datasets that correspond to\\nmultiple choice questions. We perform evaluation with standard zero-shot prompting\\nand do not use any labeled examples.\\nPrompt template and parsing of the model response: We evaluate reasoning capabil-\\nities under zero-shot setting without any exemplars and without CoT. Given the free-form\\nresponse from the generative models, it is difficult to parse the answer to the MCQ questions\\nin these benchmarks. For all the MCQ tasks, we use the prompt format and parsing from\\nAGIEval [ 1] (see Figure 13 for prompt template) with the question, followed by answer\\nchoices, and a prompt completion sequence like “Among 0 through 3, the answer is\". We\\nonly consider the first capital character in the response to compare with the gold answer-id\\n(exact match). Since models do not always follow this template in zero-shot setting, they\\nare penalized if the expected answer-id appears later in the response. We employ the same\\nparsing logic to all the models’ responses for consistency.\\n16Dataset Reference Vicuna-13B Orca-13B\\nVicuna PromptsChatGPT 92 101.5 (10.4%)\\nGPT-4 73.8 87.7 (18.9%)\\nAwesome PromptsChatGPT 86.5 98.1 (13.5%)\\nGPT-4 77.8 89.3 (1!!!!!!!!!!']\n"
     ]
    }
   ],
   "source": [
    "# Example text input\n",
    "#text_input= '### Human: What is Artificial Intelligence?  ### Assistant:'\n",
    "\n",
    "# Tokenize the text input\n",
    "input_ids = tokenizer.encode(text_input, return_tensors=\"pt\")\n",
    "\n",
    "# Create LONGMEM model\n",
    "#longmem_model = LongMEM(vocab_size, embedding_dim, hidden_dim, num_layers, memory_dim, backbone_model_name, backbone_config)\n",
    "max_length=10\n",
    "outputs = model.generate_text(input_ids,max_length)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LongMem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Piplining (Data Acquisition , Data Selection , Data Preprocessing, Data Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.corpus import gutenberg\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, Dataset,tokenizer,max_sequence_length,batch_size,sample_size):\n",
    "        self.Dataset = Dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def DataTokenization(self ,text, max_sequence_length):\n",
    "        value = self.tokenizer.encode(text)\n",
    "        padded_value= self.tokenizer.pad_sequences([value], max_sequence_length)[0]\n",
    "        return padded_value\n",
    "\n",
    "    def TrainDataPreprocess(self,Dataset,max_sequence_length ,sample_size,device):\n",
    "        # Create your dataset\n",
    "        dataset =Dataset  # Your dataset creation logic here\n",
    "        # Extract the columns 'system_prompt', 'question', and 'response'\n",
    "        system_prompts = dataset['train']['system_prompt']\n",
    "        questions = dataset['train']['question']\n",
    "        responses = dataset['train']['response']\n",
    "        # Preprocess and pad the sequences\n",
    "        preprocessed_data = []\n",
    "        for i in range(0,sample_size):\n",
    "            submax_sequence_length=int(max_sequence_length/2)\n",
    "            padded_system_prompt= self.DataTokenization(system_prompts[i], submax_sequence_length)\n",
    "            padded_question= self.DataTokenization(questions[i], submax_sequence_length)\n",
    "            padded_response= self.DataTokenization(responses[i], max_sequence_length)\n",
    "            preprocessed_data.append({\n",
    "                'system_prompt': padded_system_prompt,\n",
    "                'question': padded_question,\n",
    "                'response': padded_response\n",
    "            })\n",
    "\n",
    "        # Convert the preprocessed data to tensors\n",
    "        system_prompt_tensors = torch.tensor([item['system_prompt'] for item in preprocessed_data], dtype=torch.long)\n",
    "        question_tensors = torch.tensor([item['question'] for item in preprocessed_data], dtype=torch.long)\n",
    "        response_tensors = torch.tensor([item['response'] for item in preprocessed_data], dtype=torch.long)\n",
    "\n",
    "        # Concatenate system_prompts and questions along the appropriate dimension\n",
    "        input_ids = torch.cat((system_prompt_tensors, question_tensors), dim=1)\n",
    "        response_tensors.to(dtype=torch.long, device=device)\n",
    "        # Create DataLoader with the preprocessed tensors\n",
    "        data = torch.utils.data.TensorDataset(input_ids, response_tensors)\n",
    "        return data\n",
    "    \n",
    "    def TestDataPreprocess(self,Dataset,max_sequence_length ,sample_size,device):\n",
    "        # Create your dataset\n",
    "        dataset =Dataset  # Your dataset creation logic here\n",
    "        # Extract the columns 'system_prompt', 'question', and 'response'\n",
    "        system_prompts = dataset['test']['system_prompt']\n",
    "        questions = dataset['test']['question']\n",
    "        responses = dataset['test']['response']\n",
    "        # Preprocess and pad the sequences\n",
    "        preprocessed_data = []\n",
    "        for i in range(0,sample_size):\n",
    "            submax_sequence_length=int(max_sequence_length/2)\n",
    "            padded_system_prompt= self.DataTokenization(system_prompts[i], submax_sequence_length)\n",
    "            padded_question= self.DataTokenization(questions[i], submax_sequence_length)\n",
    "            padded_response= self.DataTokenization(responses[i], max_sequence_length)\n",
    "            preprocessed_data.append({\n",
    "                'system_prompt': padded_system_prompt,\n",
    "                'question': padded_question,\n",
    "                'response': padded_response\n",
    "            })\n",
    "\n",
    "        # Convert the preprocessed data to tensors\n",
    "        system_prompt_tensors = torch.tensor([item['system_prompt'] for item in preprocessed_data], dtype=torch.long)\n",
    "        question_tensors = torch.tensor([item['question'] for item in preprocessed_data], dtype=torch.long)\n",
    "        response_tensors = torch.tensor([item['response'] for item in preprocessed_data], dtype=torch.long)\n",
    "\n",
    "        # Concatenate system_prompts and questions along the appropriate dimension\n",
    "        input_ids = torch.cat((system_prompt_tensors, question_tensors), dim=1)\n",
    "        response_tensors.to(dtype=torch.long, device=device)\n",
    "        # Create DataLoader with the preprocessed tensors\n",
    "        data = torch.utils.data.TensorDataset(input_ids, response_tensors)\n",
    "        return data\n",
    "\n",
    "    def DataDivision(self, data, test_size=0.2, random_state=42):\n",
    "        # Split the data into training and validation sets\n",
    "        train_data, val_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # Create DataLoader for training data\n",
    "        #batch_size = 16 # Adjust as needed\n",
    "        train_dataloader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Create DataLoader for validation data\n",
    "        validation_batch_size = 2 # Adjust as needed\n",
    "        validation_dataloader = DataLoader(val_data, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_dataloader,validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/shirsh10mall___parquet/shirsh10mall--LLM_Instruct_Learning_Project_Preprocessed_Tokenized_Open_Orca_Dataset_Flan_T5-99b6087a15a77b74/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b0c7bae3504cc08ae0b57658845a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"shirsh10mall/LLM_Instruct_Learning_Project_Preprocessed_Tokenized_Open_Orca_Dataset_Flan_T5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system_prompt', 'question', 'response', 'input_ids', 'attention_mask', 'labels', 'Inputs Token length', 'Response Token length'],\n",
       "        num_rows: 430318\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['system_prompt', 'question', 'response', 'input_ids', 'attention_mask', 'labels', 'Inputs Token length', 'Response Token length'],\n",
       "        num_rows: 75939\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length=900\n",
    "sample_size=1000\n",
    "batch_size=2\n",
    "device = torch.device('cuda')\n",
    "LDS= LanguageModelDataset(dataset,tokenizer,max_sequence_length,batch_size,sample_size)\n",
    "data=LDS.TrainDataPreprocess(dataset,max_sequence_length ,sample_size,device)\n",
    "train_dataloader,validation_dataloader= LDS.DataDivision(data,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerate Library with fullyshradded data parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-21 05:42:25,950] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 05:42:27.068804: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-21 05:42:27.154720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 05:42:28.134055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate import FullyShardedDataParallelPlugin\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=False, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=False, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-18 13:18:31,433] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 13:18:31.879240: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-18 13:18:31.924362: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-18 13:18:32.920824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Accelerator with the specified parameters\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Now you can use the accelerator for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accelerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m feedforward_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m hidden_dim  \u001b[38;5;66;03m# Size of the feedforward layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Dropout rate for regularization\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m LongMEM(vocab_size, embedding_dim,num_heads, hidden_dim, num_layers, memory_dim, feedforward_size, dropout_rate)\u001b[38;5;241m.\u001b[39mto(\u001b[43maccelerator\u001b[49m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Wrap your data loaders using the accelerator\u001b[39;00m\n\u001b[1;32m     14\u001b[0m train_dataloader, validation_dataloader \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mprepare(train_dataloader, validation_dataloader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accelerator' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the LongMEM model\n",
    "# Example usage\n",
    "vocab_size = vocab_size\n",
    "embedding_dim = 768  # Embedding dimension\n",
    "num_heads = 12  # Number of attention heads\n",
    "hidden_dim = 768  # Hidden dimension of the feedforward layer\n",
    "num_layers = 12  # Number of transformer layers\n",
    "memory_dim = vocab_size  # Dimension of the memory bank\n",
    "feedforward_size = 4 * hidden_dim  # Size of the feedforward layer\n",
    "dropout_rate = 0.1  # Dropout rate for regularization\n",
    "\n",
    "model = LongMEM(vocab_size, embedding_dim,num_heads, hidden_dim, num_layers, memory_dim, feedforward_size, dropout_rate).to(accelerator.device)\n",
    "# Wrap your data loaders using the accelerator\n",
    "train_dataloader, validation_dataloader = accelerator.prepare(train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 06:46:25.997896: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-28 06:46:26.060516: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 06:46:27.196498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.autograd import profiler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, memory_dim):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.memory_dim = memory_dim\n",
    "        # You can initialize any other components or hyperparameters of your custom loss here\n",
    "        \n",
    "    def forward(self, logits, targets, memory_augmented):\n",
    "        # Compute your custom loss based on logits, targets, and memory_augmented\n",
    "        # You can use torch.nn.functional functions for loss calculations\n",
    "        \n",
    "        # Calculate the standard cross-entropy loss\n",
    "        ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        # Calculate any additional loss components based on memory_augmented and other information\n",
    "        # For example, you can compute a loss term that encourages alignment between logits and memory_augmented\n",
    "        alignment_loss = torch.mean(torch.abs(logits - memory_augmented))\n",
    "        \n",
    "        # Combine the different loss components into a final loss\n",
    "        # You can use any combination of loss terms that suits your model\n",
    "        # For example, you can simply return the cross-entropy loss combined with the alignment loss\n",
    "        # Or you can apply a weighted combination of different loss terms\n",
    "        weight_ce = 0.8  # Weight for cross-entropy loss\n",
    "        weight_alignment = 0.2  # Weight for alignment loss\n",
    "        \n",
    "        total_loss = (weight_ce * ce_loss) + (weight_alignment * alignment_loss)\n",
    "        \n",
    "        # Return the final computed loss value\n",
    "        return total_loss\n",
    "\n",
    "class LongMemTrainer(nn.Module):\n",
    "    def __init__(self, model, train_dataloader, validation_dataloader, device,learning_rate,num_training_steps,num_warmup_steps):\n",
    "        super(LongMemTrainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "        self.device = device\n",
    "\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer, self.num_warmup_steps, self.num_training_steps)\n",
    "\n",
    "        self.batch_size = None  # Set your batch size\n",
    "        self.sequence_length = None  # Set your sequence length\n",
    "        self.vocab_size = None  # Set your vocabulary size\n",
    "        self.custom_loss = CustomLoss(memory_dim)\n",
    "\n",
    "\n",
    "    def calculate_loss(logits, targets):\n",
    "        # Calculate the Cross-Entropy loss between predicted logits and target indices\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            batch = [item for item in batch]\n",
    "            input_ids, response_tensor = batch\n",
    "\n",
    "            with profiler.profile(record_shapes=True, use_cuda=True) as prof:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits,memory_augmented = self.model(input_ids)\n",
    "                responses = response_tensor.to(dtype=torch.long)\n",
    "                loss = self.custom_loss(logits, responses,memory_augmented)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in self.validation_dataloader:\n",
    "                batch = [item for item in batch]\n",
    "                input_ids, response_tensor = batch\n",
    "                logits,memory_augmented = self.model(input_ids)\n",
    "                responses = response_tensor.to(dtype=torch.long)\n",
    "                loss = self.custom_loss(logits, responses,memory_augmented)\n",
    "\n",
    "            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train_epoch(epoch)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            self.validate_epoch()\n",
    "            # Move the model's parameters and buffers to the desired device\n",
    "            self.model.to(device)\n",
    "            filepath = 'trained_model.pth'\n",
    "            torch.save(self.model.state_dict(), filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Cuda Data Parallelism with Mixed Precision Training and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 10:29:23.376603: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-28 10:29:23.426656: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-28 10:29:24.608117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.autograd import profiler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, memory_dim):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.memory_dim = memory_dim\n",
    "        # You can initialize any other components or hyperparameters of your custom loss here\n",
    "        \n",
    "    def forward(self, logits, targets, memory_augmented):\n",
    "        # Compute your custom loss based on logits, targets, and memory_augmented\n",
    "        # You can use torch.nn.functional functions for loss calculations\n",
    "        \n",
    "        # Calculate the standard cross-entropy loss\n",
    "        ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        print(\"Loss:\", ce_loss)\n",
    "        \n",
    "        # Calculate any additional loss components based on memory_augmented and other information\n",
    "        # For example, you can compute a loss term that encourages alignment between logits and memory_augmented\n",
    "        # Calculate attention scores between logits and memory_augmented\n",
    "        # Assuming memory_augmented has been appropriately reshaped for compatibility\n",
    "        attention_scores = torch.matmul(logits, memory_augmented.permute(2, 0, 1))  # Shape: [1800, 768, 2]\n",
    "\n",
    "        # Calculate alignment loss based on attention scores\n",
    "        alignment_loss = F.cross_entropy(attention_scores.view(-1, 2), targets.view(-1))\n",
    "        #print(\"alignment_loss:\", alignment_loss)\n",
    "        # Combine the different loss components into a final loss\n",
    "        # You can use any combination of loss terms that suits your model\n",
    "        # For example, you can simply return the cross-entropy loss combined with the alignment loss\n",
    "        # Or you can apply a weighted combination of different loss terms\n",
    "        weight_ce = 0.8  # Weight for cross-entropy loss\n",
    "        weight_alignment = 0.2  # Weight for alignment loss\n",
    "        \n",
    "        total_loss = (weight_ce * ce_loss) + (weight_alignment * alignment_loss)\n",
    "        \n",
    "        # Return the final computed loss value\n",
    "        return total_loss \n",
    "\n",
    "class LongMemTrainer(nn.Module):\n",
    "    def __init__(self, model, train_dataloader, validation_dataloader, device,learning_rate,num_training_steps,num_warmup_steps,memory_dim):\n",
    "        super(LongMemTrainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "        self.device = device\n",
    "\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer, self.num_warmup_steps, self.num_training_steps)\n",
    "\n",
    "        self.batch_size = None  # Set your batch size\n",
    "        self.sequence_length = None  # Set your sequence length\n",
    "        self.vocab_size = None  # Set your vocabulary size\n",
    "        self.custom_loss = CustomLoss(memory_dim)\n",
    "\n",
    "\n",
    "    def calculate_loss(self,logits, targets):\n",
    "        # Calculate the Cross-Entropy loss between predicted logits and target indices\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        \n",
    "        for batch in self.train_dataloader:\n",
    "            batch = [item.to(self.device) for item in batch]\n",
    "            input_ids, response_tensor = batch\n",
    "\n",
    "            with profiler.profile(record_shapes=True, use_cuda=True) as prof:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits,memory_augmented = self.model(input_ids)\n",
    "                responses = response_tensor.to(dtype=torch.long, device=self.device)\n",
    "                loss = self.calculate_loss(logits,responses).to(self.device)\n",
    "                print(\"Training loss:\", loss.item())\n",
    "                #loss.backward()\n",
    "                #self.optimizer.step()\n",
    "                #self.scheduler.step()\n",
    "\n",
    "            print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in self.validation_dataloader:\n",
    "                batch = [item.to(self.device) for item in batch]\n",
    "                input_ids, response_tensor = batch\n",
    "                logits,memory_augmented = self.model(input_ids)\n",
    "                responses = response_tensor.to(dtype=torch.long, device=self.device)\n",
    "                loss = self.calculate_loss(logits,responses).to(self.device)\n",
    "                print(\"Training loss:\", loss.item())\n",
    "                loss.backward()\n",
    "\n",
    "            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train_epoch(epoch)\n",
    "            self.validate_epoch()\n",
    "        filepath= 'trained_model.pth'\n",
    "        torch.save(self.model.state_dict(), 'trained_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.75 GiB (GPU 0; 23.69 GiB total capacity; 1.34 GiB already allocated; 6.76 GiB free; 1.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLongMEM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeedforward_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.75 GiB (GPU 0; 23.69 GiB total capacity; 1.34 GiB already allocated; 6.76 GiB free; 1.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "vocab_size = vocab_size\n",
    "embedding_dim = 768  # Embedding dimension\n",
    "num_heads = 12  # Number of attention heads\n",
    "hidden_dim = 768  # Hidden dimension of the feedforward layer\n",
    "num_layers = 12  # Number of transformer layers\n",
    "memory_dim = vocab_size  # Dimension of the memory bank\n",
    "feedforward_size = 4 * hidden_dim  # Size of the feedforward layer\n",
    "dropout_rate = 0.1  # Dropout rate for regularization\n",
    "learning_rate=0.1\n",
    "device = torch.device('cuda')\n",
    "model = nn.DataParallel(LongMEM(vocab_size, embedding_dim,num_heads, hidden_dim, num_layers, memory_dim, feedforward_size, dropout_rate)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m total_params, model_size_mb,size_gb  \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_size\u001b[49m(model)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal parameters in the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel size in MB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size_mb\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_model_size' is not defined"
     ]
    }
   ],
   "source": [
    "total_params, model_size_mb,size_gb  = get_model_size(model)\n",
    "print(f\"Total parameters in the model: {total_params}\")\n",
    "print(f\"Model size in MB: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create an instance of MyModelTrainer\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m LongMemTrainer(\u001b[43mmodel\u001b[49m, train_dataloader, validation_dataloader, device,learning_rate,num_training_steps,num_warmup_steps,memory_dim)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Choose an appropriate number of epochs\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "num_training_steps=2\n",
    "num_warmup_steps=2\n",
    "# Create an instance of MyModelTrainer\n",
    "trainer = LongMemTrainer(model, train_dataloader, validation_dataloader, device,learning_rate,num_training_steps,num_warmup_steps,memory_dim)\n",
    "# Train the model\n",
    "num_epochs = 10  # Choose an appropriate number of epochs\n",
    "trainer.train(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Cuda FullyShradded Data Parallelism with Mixed Precision Training and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "import os\n",
    "import argparse\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
    "    CPUOffload,\n",
    "    BackwardPrefetch,\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import (\n",
    "    size_based_auto_wrap_policy,\n",
    "    enable_wrap,\n",
    "    wrap,\n",
    ")\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "def train(model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n",
    "    model.train()\n",
    "    ddp_loss = torch.zeros(2).to(rank)\n",
    "    if sampler:\n",
    "        sampler.set_epoch(epoch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(rank), target.to(rank)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target, reduction='sum')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ddp_loss[0] += loss.item()\n",
    "        ddp_loss[1] += len(data)\n",
    "        print('Batach Training loss: {} \\tLoss: {:.6f}'.format(loss.item()))\n",
    "\n",
    "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
    "    if rank == 0:\n",
    "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))\n",
    "        \n",
    "def fsdp_main(rank, world_size,traindata,testdata):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    dataset1 = traindata\n",
    "    dataset2 = testdata\n",
    "\n",
    "    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)\n",
    "    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n",
    "    cuda_kwargs = {'num_workers': 2,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': False}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        size_based_auto_wrap_policy, min_num_params=100\n",
    "    )\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "\n",
    "    init_start_event = torch.cuda.Event(enable_timing=True)\n",
    "    init_end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    \n",
    "    # Instantiate the LongMEM model\n",
    "    # Example usage\n",
    "    vocab_size = vocab_size\n",
    "    embedding_dim = 768  # Embedding dimension\n",
    "    num_heads = 12  # Number of attention heads\n",
    "    hidden_dim = 768  # Hidden dimension of the feedforward layer\n",
    "    num_layers = 12  # Number of transformer layers\n",
    "    memory_dim = vocab_size  # Dimension of the memory bank\n",
    "    feedforward_size = 4 * hidden_dim  # Size of the feedforward layer\n",
    "    dropout_rate = 0.1  # Dropout rate for regularization\n",
    "    backbone_model_name = \"LongMem\"  # Pretrained model name, e.g., \"gpt2\"\n",
    "    learning_rate=0.1\n",
    "    gamma=1\n",
    "    epochs=10\n",
    "    model = Net().to(rank)\n",
    "\n",
    "    model = FSDP(model)\n",
    "\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    init_start_event.record()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n",
    "        test(model, rank, world_size, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    init_end_event.record()\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event) / 1000}sec\")\n",
    "        print(f\"{model}\")\n",
    "\n",
    "    if args.save_model:\n",
    "        # use a barrier to make sure training is done on all ranks\n",
    "        dist.barrier()\n",
    "        # state_dict for FSDP model is only available on Nightlies for now\n",
    "        states = model.state_dict()\n",
    "        if rank == 0:\n",
    "            torch.save(states, \"mnist_cnn.pt\")\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdp_main(0, 2,traindata,testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=0\n",
    "world_size=2\n",
    "setup(rank, world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    # Count the total number of parameters in the model\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Get the size of the model in bytes\n",
    "    model_size_bytes = total_params * 4  # Assuming 4 bytes per float32 parameter (adjust if using other types)\n",
    "    \n",
    "    # Convert to megabytes (MB) for a more readable output\n",
    "    model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    # Get the total size of model parameters and buffers\n",
    "    size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    size += sum(p.numel() for p in model.buffers())\n",
    "    \n",
    "    # Convert size from bytes to gigabytes (GB)\n",
    "    size_gb = size / (1024 ** 3)\n",
    "    return total_params, model_size_mb,size_gb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 2977692792\n",
      "Model size in MB: 11359.00 MB\n"
     ]
    }
   ],
   "source": [
    "total_params, model_size_mb,size_gb  = get_model_size(model)\n",
    "print(f\"Total parameters in the model: {total_params}\")\n",
    "print(f\"Model size in MB: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMWithMemory(\n",
       "  (memory_bank): CachedMemoryBank(\n",
       "    (embedding): Embedding(70000, 32001)\n",
       "    (memory_key): Linear(in_features=32001, out_features=32001, bias=True)\n",
       "    (memory_value): Linear(in_features=32001, out_features=32001, bias=True)\n",
       "  )\n",
       "  (transformer): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32001, 5120, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-39): 40 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
       "            (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
       "            (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
       "            (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
       "            (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
       "            (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=5120, out_features=32001, bias=False)\n",
       "  )\n",
       "  (side_net): ResidualSideNet(\n",
       "    (fc1): Linear(in_features=32001, out_features=32001, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=32001, out_features=32001, bias=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=32001, out_features=70000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary to a file\n",
    "#torch.save(model.state_dict(), \"llm_model_state_dict.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model for Harddisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.75 GiB (GPU 0; 23.69 GiB total capacity; 13.78 GiB already allocated; 90.56 MiB free; 13.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load model and its state dictionary\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLongMEM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load the state dictionary into the model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_checkpoint_epoch_0.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.75 GiB (GPU 0; 23.69 GiB total capacity; 13.78 GiB already allocated; 90.56 MiB free; 13.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define model configuration\n",
    "model_config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': 768,\n",
    "    'num_heads': 12,\n",
    "    'hidden_dim': 768,\n",
    "    'num_layers': 12,\n",
    "    'memory_dim': vocab_size,\n",
    "    'feedforward_size': 4 * 768,\n",
    "    'dropout_rate': 0.1,\n",
    "    'backbone_model_name': 'gpt2'\n",
    "}\n",
    "device = torch.device('cuda')\n",
    "# Load model and its state dictionary\n",
    "model = nn.DataParallel(LongMEM(**model_config)).to(device)\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(torch.load(\"model_checkpoint_epoch_0.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state dictionary keys:\n",
      "odict_keys(['module.frozen_llm.embedding.weight', 'module.frozen_llm.transformer_blocks.0.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.0.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.0.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.0.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.0.norm1.weight', 'module.frozen_llm.transformer_blocks.0.norm1.bias', 'module.frozen_llm.transformer_blocks.0.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.0.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.0.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.0.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.0.norm2.weight', 'module.frozen_llm.transformer_blocks.0.norm2.bias', 'module.frozen_llm.transformer_blocks.1.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.1.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.1.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.1.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.1.norm1.weight', 'module.frozen_llm.transformer_blocks.1.norm1.bias', 'module.frozen_llm.transformer_blocks.1.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.1.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.1.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.1.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.1.norm2.weight', 'module.frozen_llm.transformer_blocks.1.norm2.bias', 'module.frozen_llm.transformer_blocks.2.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.2.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.2.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.2.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.2.norm1.weight', 'module.frozen_llm.transformer_blocks.2.norm1.bias', 'module.frozen_llm.transformer_blocks.2.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.2.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.2.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.2.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.2.norm2.weight', 'module.frozen_llm.transformer_blocks.2.norm2.bias', 'module.frozen_llm.transformer_blocks.3.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.3.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.3.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.3.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.3.norm1.weight', 'module.frozen_llm.transformer_blocks.3.norm1.bias', 'module.frozen_llm.transformer_blocks.3.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.3.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.3.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.3.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.3.norm2.weight', 'module.frozen_llm.transformer_blocks.3.norm2.bias', 'module.frozen_llm.transformer_blocks.4.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.4.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.4.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.4.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.4.norm1.weight', 'module.frozen_llm.transformer_blocks.4.norm1.bias', 'module.frozen_llm.transformer_blocks.4.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.4.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.4.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.4.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.4.norm2.weight', 'module.frozen_llm.transformer_blocks.4.norm2.bias', 'module.frozen_llm.transformer_blocks.5.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.5.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.5.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.5.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.5.norm1.weight', 'module.frozen_llm.transformer_blocks.5.norm1.bias', 'module.frozen_llm.transformer_blocks.5.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.5.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.5.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.5.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.5.norm2.weight', 'module.frozen_llm.transformer_blocks.5.norm2.bias', 'module.frozen_llm.transformer_blocks.6.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.6.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.6.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.6.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.6.norm1.weight', 'module.frozen_llm.transformer_blocks.6.norm1.bias', 'module.frozen_llm.transformer_blocks.6.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.6.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.6.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.6.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.6.norm2.weight', 'module.frozen_llm.transformer_blocks.6.norm2.bias', 'module.frozen_llm.transformer_blocks.7.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.7.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.7.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.7.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.7.norm1.weight', 'module.frozen_llm.transformer_blocks.7.norm1.bias', 'module.frozen_llm.transformer_blocks.7.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.7.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.7.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.7.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.7.norm2.weight', 'module.frozen_llm.transformer_blocks.7.norm2.bias', 'module.frozen_llm.transformer_blocks.8.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.8.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.8.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.8.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.8.norm1.weight', 'module.frozen_llm.transformer_blocks.8.norm1.bias', 'module.frozen_llm.transformer_blocks.8.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.8.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.8.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.8.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.8.norm2.weight', 'module.frozen_llm.transformer_blocks.8.norm2.bias', 'module.frozen_llm.transformer_blocks.9.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.9.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.9.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.9.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.9.norm1.weight', 'module.frozen_llm.transformer_blocks.9.norm1.bias', 'module.frozen_llm.transformer_blocks.9.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.9.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.9.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.9.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.9.norm2.weight', 'module.frozen_llm.transformer_blocks.9.norm2.bias', 'module.frozen_llm.transformer_blocks.10.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.10.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.10.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.10.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.10.norm1.weight', 'module.frozen_llm.transformer_blocks.10.norm1.bias', 'module.frozen_llm.transformer_blocks.10.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.10.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.10.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.10.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.10.norm2.weight', 'module.frozen_llm.transformer_blocks.10.norm2.bias', 'module.frozen_llm.transformer_blocks.11.self_attention.in_proj_weight', 'module.frozen_llm.transformer_blocks.11.self_attention.in_proj_bias', 'module.frozen_llm.transformer_blocks.11.self_attention.out_proj.weight', 'module.frozen_llm.transformer_blocks.11.self_attention.out_proj.bias', 'module.frozen_llm.transformer_blocks.11.norm1.weight', 'module.frozen_llm.transformer_blocks.11.norm1.bias', 'module.frozen_llm.transformer_blocks.11.feedforward.0.weight', 'module.frozen_llm.transformer_blocks.11.feedforward.0.bias', 'module.frozen_llm.transformer_blocks.11.feedforward.2.weight', 'module.frozen_llm.transformer_blocks.11.feedforward.2.bias', 'module.frozen_llm.transformer_blocks.11.norm2.weight', 'module.frozen_llm.transformer_blocks.11.norm2.bias', 'module.frozen_llm.linear.weight', 'module.frozen_llm.linear.bias', 'module.memory_bank.embedding.weight', 'module.memory_bank.memory_key.weight', 'module.memory_bank.memory_key.bias', 'module.memory_bank.memory_value.weight', 'module.memory_bank.memory_value.bias', 'module.side_net.fc1.weight', 'module.side_net.fc1.bias', 'module.side_net.fc2.weight', 'module.side_net.fc2.bias', 'module.memory_fusion.linear_query.weight', 'module.memory_fusion.linear_query.bias'])\n",
      "\n",
      "Current model keys:\n",
      "odict_keys(['frozen_llm.embedding.weight', 'frozen_llm.transformer_blocks.0.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.0.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.0.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.0.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.0.norm1.weight', 'frozen_llm.transformer_blocks.0.norm1.bias', 'frozen_llm.transformer_blocks.0.feedforward.0.weight', 'frozen_llm.transformer_blocks.0.feedforward.0.bias', 'frozen_llm.transformer_blocks.0.feedforward.2.weight', 'frozen_llm.transformer_blocks.0.feedforward.2.bias', 'frozen_llm.transformer_blocks.0.norm2.weight', 'frozen_llm.transformer_blocks.0.norm2.bias', 'frozen_llm.transformer_blocks.1.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.1.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.1.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.1.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.1.norm1.weight', 'frozen_llm.transformer_blocks.1.norm1.bias', 'frozen_llm.transformer_blocks.1.feedforward.0.weight', 'frozen_llm.transformer_blocks.1.feedforward.0.bias', 'frozen_llm.transformer_blocks.1.feedforward.2.weight', 'frozen_llm.transformer_blocks.1.feedforward.2.bias', 'frozen_llm.transformer_blocks.1.norm2.weight', 'frozen_llm.transformer_blocks.1.norm2.bias', 'frozen_llm.transformer_blocks.2.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.2.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.2.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.2.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.2.norm1.weight', 'frozen_llm.transformer_blocks.2.norm1.bias', 'frozen_llm.transformer_blocks.2.feedforward.0.weight', 'frozen_llm.transformer_blocks.2.feedforward.0.bias', 'frozen_llm.transformer_blocks.2.feedforward.2.weight', 'frozen_llm.transformer_blocks.2.feedforward.2.bias', 'frozen_llm.transformer_blocks.2.norm2.weight', 'frozen_llm.transformer_blocks.2.norm2.bias', 'frozen_llm.transformer_blocks.3.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.3.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.3.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.3.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.3.norm1.weight', 'frozen_llm.transformer_blocks.3.norm1.bias', 'frozen_llm.transformer_blocks.3.feedforward.0.weight', 'frozen_llm.transformer_blocks.3.feedforward.0.bias', 'frozen_llm.transformer_blocks.3.feedforward.2.weight', 'frozen_llm.transformer_blocks.3.feedforward.2.bias', 'frozen_llm.transformer_blocks.3.norm2.weight', 'frozen_llm.transformer_blocks.3.norm2.bias', 'frozen_llm.transformer_blocks.4.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.4.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.4.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.4.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.4.norm1.weight', 'frozen_llm.transformer_blocks.4.norm1.bias', 'frozen_llm.transformer_blocks.4.feedforward.0.weight', 'frozen_llm.transformer_blocks.4.feedforward.0.bias', 'frozen_llm.transformer_blocks.4.feedforward.2.weight', 'frozen_llm.transformer_blocks.4.feedforward.2.bias', 'frozen_llm.transformer_blocks.4.norm2.weight', 'frozen_llm.transformer_blocks.4.norm2.bias', 'frozen_llm.transformer_blocks.5.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.5.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.5.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.5.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.5.norm1.weight', 'frozen_llm.transformer_blocks.5.norm1.bias', 'frozen_llm.transformer_blocks.5.feedforward.0.weight', 'frozen_llm.transformer_blocks.5.feedforward.0.bias', 'frozen_llm.transformer_blocks.5.feedforward.2.weight', 'frozen_llm.transformer_blocks.5.feedforward.2.bias', 'frozen_llm.transformer_blocks.5.norm2.weight', 'frozen_llm.transformer_blocks.5.norm2.bias', 'frozen_llm.transformer_blocks.6.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.6.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.6.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.6.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.6.norm1.weight', 'frozen_llm.transformer_blocks.6.norm1.bias', 'frozen_llm.transformer_blocks.6.feedforward.0.weight', 'frozen_llm.transformer_blocks.6.feedforward.0.bias', 'frozen_llm.transformer_blocks.6.feedforward.2.weight', 'frozen_llm.transformer_blocks.6.feedforward.2.bias', 'frozen_llm.transformer_blocks.6.norm2.weight', 'frozen_llm.transformer_blocks.6.norm2.bias', 'frozen_llm.transformer_blocks.7.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.7.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.7.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.7.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.7.norm1.weight', 'frozen_llm.transformer_blocks.7.norm1.bias', 'frozen_llm.transformer_blocks.7.feedforward.0.weight', 'frozen_llm.transformer_blocks.7.feedforward.0.bias', 'frozen_llm.transformer_blocks.7.feedforward.2.weight', 'frozen_llm.transformer_blocks.7.feedforward.2.bias', 'frozen_llm.transformer_blocks.7.norm2.weight', 'frozen_llm.transformer_blocks.7.norm2.bias', 'frozen_llm.transformer_blocks.8.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.8.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.8.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.8.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.8.norm1.weight', 'frozen_llm.transformer_blocks.8.norm1.bias', 'frozen_llm.transformer_blocks.8.feedforward.0.weight', 'frozen_llm.transformer_blocks.8.feedforward.0.bias', 'frozen_llm.transformer_blocks.8.feedforward.2.weight', 'frozen_llm.transformer_blocks.8.feedforward.2.bias', 'frozen_llm.transformer_blocks.8.norm2.weight', 'frozen_llm.transformer_blocks.8.norm2.bias', 'frozen_llm.transformer_blocks.9.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.9.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.9.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.9.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.9.norm1.weight', 'frozen_llm.transformer_blocks.9.norm1.bias', 'frozen_llm.transformer_blocks.9.feedforward.0.weight', 'frozen_llm.transformer_blocks.9.feedforward.0.bias', 'frozen_llm.transformer_blocks.9.feedforward.2.weight', 'frozen_llm.transformer_blocks.9.feedforward.2.bias', 'frozen_llm.transformer_blocks.9.norm2.weight', 'frozen_llm.transformer_blocks.9.norm2.bias', 'frozen_llm.transformer_blocks.10.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.10.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.10.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.10.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.10.norm1.weight', 'frozen_llm.transformer_blocks.10.norm1.bias', 'frozen_llm.transformer_blocks.10.feedforward.0.weight', 'frozen_llm.transformer_blocks.10.feedforward.0.bias', 'frozen_llm.transformer_blocks.10.feedforward.2.weight', 'frozen_llm.transformer_blocks.10.feedforward.2.bias', 'frozen_llm.transformer_blocks.10.norm2.weight', 'frozen_llm.transformer_blocks.10.norm2.bias', 'frozen_llm.transformer_blocks.11.self_attention.in_proj_weight', 'frozen_llm.transformer_blocks.11.self_attention.in_proj_bias', 'frozen_llm.transformer_blocks.11.self_attention.out_proj.weight', 'frozen_llm.transformer_blocks.11.self_attention.out_proj.bias', 'frozen_llm.transformer_blocks.11.norm1.weight', 'frozen_llm.transformer_blocks.11.norm1.bias', 'frozen_llm.transformer_blocks.11.feedforward.0.weight', 'frozen_llm.transformer_blocks.11.feedforward.0.bias', 'frozen_llm.transformer_blocks.11.feedforward.2.weight', 'frozen_llm.transformer_blocks.11.feedforward.2.bias', 'frozen_llm.transformer_blocks.11.norm2.weight', 'frozen_llm.transformer_blocks.11.norm2.bias', 'frozen_llm.linear.weight', 'frozen_llm.linear.bias', 'memory_bank.embedding.weight', 'memory_bank.memory_key.weight', 'memory_bank.memory_key.bias', 'memory_bank.memory_value.weight', 'memory_bank.memory_value.bias', 'side_net.fc1.weight', 'side_net.fc1.bias', 'side_net.fc2.weight', 'side_net.fc2.bias', 'memory_fusion.linear_query.weight', 'memory_fusion.linear_query.bias'])\n"
     ]
    }
   ],
   "source": [
    "# Print keys in saved state dictionary\n",
    "print(\"Saved state dictionary keys:\")\n",
    "print(loaded_model_state_dict.keys())\n",
    "\n",
    "# Print keys in current model\n",
    "print(\"\\nCurrent model keys:\")\n",
    "print(model.state_dict().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the fully trained model's state dictionary\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#loaded_model_state_dict = torch.load(\"fully_trained_model.pth\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#model.load_state_dict(loaded_model_state_dict)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Optionally, load specific checkpoints from the model_checkpoints dictionary\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Example: Load the checkpoint from epoch 3\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loaded_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_checkpoint_epoch_0.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m epoch_3_state_dict \u001b[38;5;241m=\u001b[39m loaded_checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(epoch_3_state_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the fully trained model's state dictionary\n",
    "#loaded_model_state_dict = torch.load(\"fully_trained_model.pth\")\n",
    "#model.load_state_dict(loaded_model_state_dict)\n",
    "\n",
    "# Optionally, load specific checkpoints from the model_checkpoints dictionary\n",
    "# Example: Load the checkpoint from epoch 3\n",
    "loaded_checkpoint = torch.load(\"model_checkpoint_epoch_0.pth\")\n",
    "epoch_3_state_dict = loaded_checkpoint[\"epoch_3\"]\n",
    "model.load_state_dict(epoch_3_state_dict)\n",
    "\n",
    "# Now the model is loaded with the saved state dictionary\n",
    "\n",
    "# You can use the loaded model for inference or further training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "        num_pages = pdf_reader.numPages\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.getPage(page_num)\n",
    "            text += page.extractText()\n",
    "\n",
    "    return text\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    pdf_file_path = \"demodoc.pdf\"\n",
    "#    extracted_text = pdf_to_text(pdf_file_path)\n",
    "#    print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = \"demodoc.pdf\"\n",
    "extracted_text = pdf_to_text(pdf_file_path)\n",
    "# Example text input\n",
    "text_input = extracted_text[1:50000]\n",
    "# Tokenize the input text and convert to input_ids\n",
    "tokens = tokenizer.encode(text_input)\n",
    "#input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(\"tokens:\")\n",
    "print(tokens)\n",
    "print(\"Input IDs:\")\n",
    "print(input_ids)\n",
    "print(\"Input IDs shape:\")\n",
    "print(input_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_output  tensor([[    1,   364,  1113, 29901, 20018,   573, 29257,   515, 26596,    13,\n",
      "          1252,  9018,   362,  3201,   778,   310,   402,  7982, 29899, 29946,\n",
      "            13,  4035,  7308, 29878,   532,   341,  2679,  2276,  1324, 29872,\n",
      "         31083, 30087, 29892,   826,   513,   314,  4573,   336, 31083,    13,\n",
      "         29954,   273, 12094,   435,  1450,   801,   279, 29892, 24246,  1175,\n",
      "          4059,   279, 14625, 29892,  7904,   333,  3793,   574, 29875, 29892,\n",
      "          9070,  2168,   319, 13829,   284,  8083,    13, 11277, 10550,    13,\n",
      "          9118,    13,  4789,   296,  5925,   756, 21309,   373,   427,  5403,\n",
      "          3277,   278,  2117,  3097,   310,  7968,  4733,    13, 20678,   527,\n",
      "          7018,  6509, 29892, 11580,   373,   278, 14391,  5759,   491,  2919,\n",
      "            13, 11940,   362,  4733,   313, 29931, 22192, 29879,   467,   319,\n",
      "          1353,   310,  5626, 10879,   278, 11029,   310,  1438,    13,  9794,\n",
      "         29892,   364,  9776,   515,  9078,   527,  7018, 18470,   515,  4091,\n",
      "           340,   365, 22192, 14391, 29936,    13,  9278,  6287,  3632, 23724,\n",
      "          6694,   848, 29936,   322,  1556,   451,  2197,   263, 10225,   310,\n",
      "         12912, 20657,    13, 24219,   362,  9819,   297,   975,   342,   326,\n",
      "          1218,   278,  2319,  1904, 30010, 29879,  2117,  3097,   408,   896,\n",
      "            13, 29873,   355,   304,  5110,   304,   527, 10388,   278,  3114,\n",
      "         29892,   541,   451,   278, 24481,  1889,   310,   365, 22192, 29879,\n",
      "           869,  1763,    13,  7328,  1438, 18066,   267, 29892,   591,  2693,\n",
      "          1394,  1113, 29892,   263, 29871, 29896, 29941, 29899, 29890,   453,\n",
      "           291,  3443,  1904,    13,  5747, 24298,  1983,   304,   527, 10388,\n",
      "           278, 24481,  1889,   310,   365, 22192, 29879, 29889,  1394,  1113,\n",
      "         24298,  1983,   515,    13,  4018, 18470,   515,   402,  7982, 29899,\n",
      "         29946,  3704,  8252, 26695, 29936,  4331, 29899,  1609, 29899, 10568,\n",
      "          2714,    13,  5014,   267, 29936,   322,   916,  4280, 11994, 29892,\n",
      "          1410,  2618,   491, 15703,   408,  1039]])\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum length you want to generate\n",
    "max_length = 100\n",
    "\n",
    "# Generate text using the function with GPU handling\n",
    "generated_output = generate_text_with_gpu_handling(model, input_ids, max_length)\n",
    "print(\"generated_output \",generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#modelclass = LLMmodel(modelname)\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(modelname,trust_remote_code=True)\n",
    "pdf_file_path = \"demodoc.pdf\"\n",
    "#extracted_text = pdf_to_text(pdf_file_path)\n",
    "# Example text input\n",
    "#text_input = extracted_text[1:3000]\n",
    "#text_input=\"What is Vicuna?\"\n",
    "# Tokenize the text input\n",
    "user_input=\"What is Artificial Intelligence?\"\n",
    "text_input = f\"### Human QUERY: {user_input} \\n### Assistant:\"\n",
    "input_ids = tokenizer.encode(text_input)\n",
    "max_length=50\n",
    "Output_ids=model.generate_text(input_ids, max_length)\n",
    "Output_ids\n",
    "#tokenizer.decode(token_id.item(), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratedText: ### Human QUERY: What is Artificial Intelligence? \n",
      "### Assistant:县\n"
     ]
    }
   ],
   "source": [
    "# Convert the tensor to a list\n",
    "decoded_list = Output_ids.tolist()\n",
    "\n",
    "# Decode the list using the GPT-2 tokenizer\n",
    "decoded_text = tokenizer.decode(decoded_list[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the decoded text\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
